{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/boudinfl/pke.git\n",
      "  Cloning https://github.com/boudinfl/pke.git to c:\\users\\xavmo\\appdata\\local\\temp\\pip-req-build-i1zp42f6\n",
      "  Resolved https://github.com/boudinfl/pke.git to commit 69871ffdb720b83df23684fea53ec8776fd87e63\n",
      "Requirement already satisfied: nltk in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (3.7)\n",
      "Requirement already satisfied: networkx in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (2.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.0.2)\n",
      "Requirement already satisfied: unidecode in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.2.0)\n",
      "Requirement already satisfied: future in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (0.18.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.1.0)\n",
      "Collecting spacy>=3.2.3\n",
      "  Downloading spacy-3.7.5-cp39-cp39-win_amd64.whl (12.2 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (2.11.3)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp39-cp39-win_amd64.whl (25 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (21.3)\n",
      "Collecting thinc<8.3.0,>=8.2.2\n",
      "  Downloading thinc-8.2.4-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp39-cp39-win_amd64.whl (122 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp39-cp39-win_amd64.whl (483 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (2.27.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.7.3-py3-none-any.whl (409 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (4.64.0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (61.2.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "Collecting language-data>=1.2\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "Collecting marisa-trie>=0.7.7\n",
      "  Downloading marisa_trie-1.2.0-cp39-cp39-win_amd64.whl (152 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy>=3.2.3->pke==2.0.0) (3.0.4)\n",
      "Collecting pydantic-core==2.18.4\n",
      "  Downloading pydantic_core-2.18.4-cp39-none-win_amd64.whl (1.9 MB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting typing-extensions>=4.6.1\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (1.26.9)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/boudinfl/pke.git 'C:\\Users\\xavmo\\AppData\\Local\\Temp\\pip-req-build-i1zp42f6'\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "jupyter-server 1.13.5 requires pywinpty<2; os_name == \"nt\", but you have pywinpty 2.0.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.3)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy>=3.2.3->pke==2.0.0) (0.4.4)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (8.0.4)\n",
      "Collecting rich>=10.11.0\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1\n",
      "  Downloading smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (1.12.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (2.0.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from nltk->pke==2.0.0) (2022.3.15)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from scikit-learn->pke==2.0.0) (2.2.0)\n",
      "Building wheels for collected packages: pke\n",
      "  Building wheel for pke (setup.py): started\n",
      "  Building wheel for pke (setup.py): finished with status 'done'\n",
      "  Created wheel for pke: filename=pke-2.0.0-py3-none-any.whl size=6161142 sha256=3ce9f69aa1d4e248daa516120f6119948394f6501b2427d7697449fca896b064\n",
      "  Stored in directory: C:\\Users\\xavmo\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-bmrn1f1u\\wheels\\d5\\46\\97\\85535b5b449f70b6a3c8d1138ce8587345876891e25bfe7954\n",
      "Successfully built pke\n",
      "Installing collected packages: typing-extensions, mdurl, pygments, pydantic-core, markdown-it-py, colorama, catalogue, annotated-types, srsly, shellingham, rich, pydantic, murmurhash, marisa-trie, cymem, wasabi, typer, smart-open, preshed, language-data, confection, cloudpathlib, blis, weasel, thinc, spacy-loggers, spacy-legacy, langcodes, spacy, pke\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Uninstalling typing-extensions-4.1.1:\n",
      "      Successfully uninstalled typing-extensions-4.1.1\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n",
      "      Successfully uninstalled Pygments-2.11.2\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 5.1.0\n",
      "    Uninstalling smart-open-5.1.0:\n",
      "      Successfully uninstalled smart-open-5.1.0\n",
      "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.18.1 colorama-0.4.6 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.10 pke-2.0.0 preshed-3.0.9 pydantic-2.7.3 pydantic-core-2.18.4 pygments-2.18.0 rich-13.7.1 shellingham-1.5.4 smart-open-7.0.4 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.4 typer-0.12.3 typing-extensions-4.12.2 wasabi-1.1.3 weasel-0.4.1\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "Requirement already satisfied: six in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=214d2e7ad772043ff6f44d866a043b5fed6cd83b3eacbeb028aecea6f11b303b\n",
      "  Stored in directory: c:\\users\\xavmo\\appdata\\local\\pip\\cache\\wheels\\d1\\c1\\d9\\7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/boudinfl/pke.git\n",
    "!pip install langdetect\n",
    "!pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies nécessaires\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from langdetect import detect\n",
    "import json\n",
    "import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "import pke\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation d'un corpus : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre laboratoire comprend de nombreux chercheurs et chercheuses, nous décidons donc d'apporter par HAL toutes les publications de ceux-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de publications récupérées : 12388\n"
     ]
    }
   ],
   "source": [
    "# Script de Waly modifié.\n",
    "# Liste des noms d'auteurs que vous souhaitez rechercher\n",
    "auteurs = [\n",
    "    \"Anne-Virginie Salsac\",\n",
    "    \"Dan Istrate\",\n",
    "    \"Eric Leclerc\",\n",
    "    \"Julien \\\"Le Duigou\\\"\",\n",
    "    \"Marie-Christine \\\"Ho Ba Tho\\\"\",\n",
    "    \"Sofiane Boudaoud\",\n",
    "    \"Glenn Roe\",\n",
    "    \"Motasem ALRAHABI\",\n",
    "    \"Arnaud Latil\",\n",
    "    \"Christian NERI\",\n",
    "    \"Clément Mabi\",\n",
    "    \"David Flacher\",\n",
    "    \"M. Shawky\",\n",
    "    \"Serge Bouchardon\",\n",
    "    \"Harry Sokol\",\n",
    "    \"Bérangère Bihan-Avalle\",\n",
    "    \"Caroline Marti\",\n",
    "    \"Laurent Petit\",\n",
    "    \"Pierre-Carl Langlais\",\n",
    "    \"David Klatzmann\",\n",
    "    \"Raphael Gavazzi\",\n",
    "    \"Benjamin Wandelt\",\n",
    "    \"christophe pichon\",\n",
    "    \"Guilhem Lavaux\",\n",
    "    \"Henry Joy McCracken\",\n",
    "    \"Kumiko Kotera\",\n",
    "    \"Yohan Dubois\",\n",
    "    \"A. Marco Saitta\",\n",
    "    \"Dirk Stratmann\",\n",
    "    \"Guillaume Ferlat\",\n",
    "    \"Slavica Jonic\",\n",
    "    \"Alex Chin\",\n",
    "    \"Fabrice Carrat\",\n",
    "    \"Pierre-Yves Boëlle\",\n",
    "    \"Renaud Piarroux\",\n",
    "    \"Christophe Guillotel-Nothmann\",\n",
    "    \"Jean-Marc Chouvel\",\n",
    "    \"Nicolas Obin\",\n",
    "    \"Philippe Esling\",\n",
    "    \"Alexandre Coninx\",\n",
    "    \"Baptiste Caramiaux\",\n",
    "    \"Benjamin Piwowarski\",\n",
    "    \"Catherine Achard\",\n",
    "    \"Catherine Pelachaud\",\n",
    "    \"Gilles Bailly\",\n",
    "    \"Jérôme Szewczyk\",\n",
    "    \"Kevin Bailly\",\n",
    "    \"Laure Soulier\",\n",
    "    \"Marie-Aude Vitrani\",\n",
    "    \"Matthieu Cord\",\n",
    "    \"Mehdi Khamassi\",\n",
    "    \"Mohamed CHETOUANI\",\n",
    "    \"Nathanaël Jarrassé\",\n",
    "    \"Nicolas Bredeche\",\n",
    "    \"Nicolas Perrin-Gilbert\",\n",
    "    \"Nicolas Thome\",\n",
    "    \"Olivier Schwander\",\n",
    "    \"Olivier Sigaud\",\n",
    "    \"Pascal Morin\",\n",
    "    \"Pierre Bessière\",\n",
    "    \"Sinan Haliyo\",\n",
    "    \"Stéphane Doncieux\",\n",
    "    \"Alessandra Carbone\",\n",
    "    \"Elodie Laine\",\n",
    "    \"Martin Weigt\",\n",
    "    \"Benoit Semelin\",\n",
    "    \"Emeric Bron\",\n",
    "    \"Emmanuel Bertin\",\n",
    "    \"Françoise Combes\",\n",
    "    \"Maryvonne Gerin\",\n",
    "    \"Philippe Salomé\",\n",
    "    \"Baptiste Cecconi\",\n",
    "    \"Philippe Zarka\",\n",
    "    \"Ferdinand Dhombres\",\n",
    "    \"Jean Charlet\",\n",
    "    \"Xavier Tannier\",\n",
    "    \"Amal \\\"El Fallah Seghrouchni\\\"\",\n",
    "    \"Andrea Pinna\",\n",
    "    \"Antoine Miné\",\n",
    "    \"Béatrice Bérard\",\n",
    "    \"Bertrand Granado\",\n",
    "    \"Bruno Escoffier\",\n",
    "    \"Carola Doerr\",\n",
    "    \"Christoph Dürr\",\n",
    "    \"Christophe Denis\",\n",
    "    \"Christophe Marsala\",\n",
    "    \"Colette Faucher\",\n",
    "    \"Emmanuel HYON\",\n",
    "    \"Emmanuelle Encrenaz-Tiphène\",\n",
    "    \"Evripidis Bampis\",\n",
    "    \"Fanny Pascual\",\n",
    "    \"Haralampos Stratigopoulos\",\n",
    "    \"Jean-Daniel Kant\",\n",
    "    \"Jean-Gabriel Ganascia\",\n",
    "    \"Jean-Noël Vittaut\",\n",
    "    \"Lionel Tabourier\",\n",
    "    \"Maria Potop-Butucaru\",\n",
    "    \"Matthieu Latapy\",\n",
    "    \"Nicolas MAUDET\",\n",
    "    \"Olivier Spanjaard\",\n",
    "    \"Patrice Perny\",\n",
    "    \"Patrick Gallinari\",\n",
    "    \"Sébastien Tixeuil\",\n",
    "    \"Spyros Angelopoulos\",\n",
    "    \"Stéphane Gançarski\",\n",
    "    \"Vanda Luengo\",\n",
    "    \"Vincent Guigue\",\n",
    "    \"Bruno Despres\",\n",
    "    \"Frédéric Nataf\",\n",
    "    \"Julien Brajard\",\n",
    "    \"Sylvie Thiria\",\n",
    "    \"Catherine Matias\",\n",
    "    \"Charlotte Dion-Blanc\",\n",
    "    \"Claire Boyer\",\n",
    "    \"Gérard Biau\",\n",
    "    \"Gregory Nuel\",\n",
    "    \"Idris Kharroubi\",\n",
    "    \"Maud Thomas\",\n",
    "    \"Maxime Sangnier\",\n",
    "    \"Olivier Lopez\",\n",
    "    \"Sylvain Le-Corff\",\n",
    "    \"Tabea Rebafka\",\n",
    "    \"Benjamin Fuks\",\n",
    "    \"Stéphane Mottelet\",\n",
    "    \"Tien-Tuan Dao\",\n",
    "    \"julien mozziconacci\",\n",
    "    \"Nicolas Aunai\",\n",
    "    \"Thierry Dufour\",\n",
    "    \"Abdenour Hadid\",\n",
    "    \"Benjamin Quost\",\n",
    "    \"Bruno Toupance\",\n",
    "    \"Dominique Lenne\",\n",
    "    \"Evelyne Heyer\",\n",
    "    \"Franz Manni\",\n",
    "    \"Grace Younes\",\n",
    "    \"Lama Tarsissi\",\n",
    "    \"Marie-Hélène (Mylène) Masson\",\n",
    "    \"Marie-Hélène Abel\",\n",
    "    \"Nathalie Martial-Braz\",\n",
    "    \"Nicolas Patin\",\n",
    "    \"Philippe Bonnifait\",\n",
    "    \"Philippe Boulanger\",\n",
    "    \"Philippe Trigano\",\n",
    "    \"Raed Abu Zitar\",\n",
    "    \"Samuel F. Feng\",\n",
    "    \"Sébastien Destercke\",\n",
    "    \"Tanujit Chakraborty\",\n",
    "    \"Yves Grandvalet\",\n",
    "    \"Zoheir ABOURA\"\n",
    "]\n",
    "\n",
    "# Initialisez une liste vide pour stocker les métadonnées des publications\n",
    "publications = []\n",
    "\n",
    "# URL de base de l'API HAL\n",
    "base_url = \"https://api.archives-ouvertes.fr/search/\"\n",
    "\n",
    "# Parcourez la liste des auteurs et récupérez leurs publications\n",
    "for auteur in auteurs:\n",
    "    params = {\n",
    "        \"q\": f'authFullName_s:\"{auteur}\"',\n",
    "        \"fl\": \"authFullName_s,authIdHal_i,authIdHal_s,title_s,halId_s,producedDateY_i,doiId_s,abstract_s,uri_s,domain_s,keyword_s\",\n",
    "        \"rows\": 10000  # Augmentez le nombre de lignes si nécessaire\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        publications.extend(data[\"response\"][\"docs\"])\n",
    "    else:\n",
    "        print(f\"Erreur pour l'auteur {auteur}: {response.status_code} - {response.text}\")\n",
    "\n",
    "# Créez un DataFrame Pandas à partir des métadonnées des publications\n",
    "column_order = [\"authFullName_s\", \"authIdHal_i\", \"authIdHal_s\", \"title_s\", \"halId_s\", \"producedDateY_i\", \"doiId_s\", \"keyword_s\", \"abstract_s\", \"uri_s\", \"domain_s\"]\n",
    "\n",
    "df = pd.DataFrame(publications, columns= column_order)\n",
    "\n",
    "# Sauvegardez le DataFrame dans un fichier CSV\n",
    "#df.to_csv(\"publications_hal_scai_complet.csv\", index=False)\n",
    "\n",
    "# Affichez le nombre total de publications récupérées\n",
    "print(f\"Nombre total de publications récupérées : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publications restantes après nettoyage : 4206\n"
     ]
    }
   ],
   "source": [
    "# Conversion les listes en chaînes dans la colonne \"title_s\"\n",
    "df[\"title_s\"] = df[\"title_s\"].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Suppression des doublons\n",
    "df = df.drop_duplicates(subset=\"title_s\")\n",
    "dfp = df.dropna(subset=[\"abstract_s\", \"keyword_s\"])\n",
    "print(f\"Publications restantes après nettoyage : {len(dfp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Détection de la langue terminée \n",
      "\n",
      "Nombre d'articles anglais :\n",
      "3547\n",
      "Nombre d'articles francais :\n",
      "654\n",
      "Nombre d'articles allemand :\n",
      "2\n",
      "Index des articles avec langue inconnue\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "dfp = dfp.copy()\n",
    "dfp['language'] = 'unknown'\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "def lang(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i,'language'] = detect_language(str(df.loc[i,'abstract_s']))\n",
    "    print(\"Détection de la langue terminée \\n\")\n",
    "    return df\n",
    "\n",
    "dfp = lang(dfp)\n",
    "\n",
    "def counter(df):\n",
    "    counts = df['language'].value_counts()\n",
    "    print(\"Nombre d'articles anglais :\")\n",
    "    print(counts.get('en', 0))\n",
    "\n",
    "    print(\"Nombre d'articles francais :\")\n",
    "    print(counts.get('fr', 0))\n",
    "\n",
    "    print(\"Nombre d'articles allemand :\")\n",
    "    print(counts.get('de', 0))\n",
    "\n",
    "    print(\"Index des articles avec langue inconnue\")\n",
    "    print(df[df['language'] == 'UNKNOWN'].index.tolist())\n",
    "    return\n",
    "\n",
    "counter(dfp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cependant, nous remarquons que les métadonnées des publications par HAL ne sont pas aussi complètes que celles d'OpenAlex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['meta', 'results', 'group_by'])\n",
      "dict_keys(['id', 'doi', 'title', 'display_name', 'publication_year', 'publication_date', 'ids', 'language', 'primary_location', 'type', 'type_crossref', 'indexed_in', 'open_access', 'authorships', 'countries_distinct_count', 'institutions_distinct_count', 'corresponding_author_ids', 'corresponding_institution_ids', 'apc_list', 'apc_paid', 'has_fulltext', 'cited_by_count', 'cited_by_percentile_year', 'biblio', 'is_retracted', 'is_paratext', 'primary_topic', 'topics', 'keywords', 'concepts', 'mesh', 'locations_count', 'locations', 'best_oa_location', 'sustainable_development_goals', 'grants', 'datasets', 'versions', 'referenced_works_count', 'referenced_works', 'related_works', 'ngrams_url', 'abstract_inverted_index', 'cited_by_api_url', 'counts_by_year', 'updated_date', 'created_date'])\n"
     ]
    }
   ],
   "source": [
    "# ID de l'auteur\n",
    "author_id = \"a5007814380\"  \n",
    "\n",
    "# URL de l'API\n",
    "url = f\"https://api.openalex.org/works?filter=author.id:{author_id}\"\n",
    "\n",
    "# Faire la requête à l'API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Vérifier que la requête a réussi\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    # Conversion de la réponse en JSON\n",
    "    data = json.loads(response.text)\n",
    "    print(data.keys())\n",
    "    i=1\n",
    "    for work in data['results'] : \n",
    "        if i==1 :\n",
    "            print(work.keys())\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID de l'auteur\n",
    "author_id = \"a5007814380\"  \n",
    "\n",
    "# URL de l'API\n",
    "url = f\"https://api.openalex.org/works?filter=author.id:{author_id}\"\n",
    "\n",
    "# Faire la requête à l'API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Vérifier que la requête a réussi\n",
    "if response.status_code == 200:\n",
    "    # Conversion de la réponse en JSON\n",
    "    data = json.loads(response.text)\n",
    "\n",
    "    # Sélection les métadonnées précises\n",
    "    selected_data = []\n",
    "    \n",
    "    for work in data['results']:\n",
    "        \n",
    "        author_ids = []\n",
    "        if 'authorships' in work: \n",
    "            for authorship in work['authorships']:\n",
    "                # Ajoutez l'identifiant de l'auteur à la liste 'author_ids'\n",
    "                author_ids.append(authorship[\"author\"][\"id\"])\n",
    "                \n",
    "        selected_data.append({\n",
    "            'id' : work['id'],\n",
    "            'title': work['title'],\n",
    "            'authors_id' : author_ids, \n",
    "            'referenced_works': work['referenced_works'], \n",
    "            'related_works': work['related_works'], \n",
    "            'cited_by_api_url': work['cited_by_api_url'],    \n",
    "        })\n",
    "\n",
    "    # Conversion des données sélectionnées en DataFrame\n",
    "    df = pd.DataFrame(selected_data)\n",
    "\n",
    "    # Exportation du DataFrame en CSV\n",
    "    df.to_csv('meta_pub_oa.csv', index=False)\n",
    "else:\n",
    "    print(f\"Erreur : {response.status_code}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
