{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/boudinfl/pke.git\n",
      "  Cloning https://github.com/boudinfl/pke.git to c:\\users\\xavmo\\appdata\\local\\temp\\pip-req-build-i1zp42f6\n",
      "  Resolved https://github.com/boudinfl/pke.git to commit 69871ffdb720b83df23684fea53ec8776fd87e63\n",
      "Requirement already satisfied: nltk in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (3.7)\n",
      "Requirement already satisfied: networkx in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (2.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.0.2)\n",
      "Requirement already satisfied: unidecode in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.2.0)\n",
      "Requirement already satisfied: future in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (0.18.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.1.0)\n",
      "Collecting spacy>=3.2.3\n",
      "  Downloading spacy-3.7.5-cp39-cp39-win_amd64.whl (12.2 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (2.11.3)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp39-cp39-win_amd64.whl (25 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (21.3)\n",
      "Collecting thinc<8.3.0,>=8.2.2\n",
      "  Downloading thinc-8.2.4-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp39-cp39-win_amd64.whl (122 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp39-cp39-win_amd64.whl (483 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (2.27.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.7.3-py3-none-any.whl (409 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (4.64.0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (61.2.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "Collecting language-data>=1.2\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "Collecting marisa-trie>=0.7.7\n",
      "  Downloading marisa_trie-1.2.0-cp39-cp39-win_amd64.whl (152 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy>=3.2.3->pke==2.0.0) (3.0.4)\n",
      "Collecting pydantic-core==2.18.4\n",
      "  Downloading pydantic_core-2.18.4-cp39-none-win_amd64.whl (1.9 MB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting typing-extensions>=4.6.1\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (1.26.9)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/boudinfl/pke.git 'C:\\Users\\xavmo\\AppData\\Local\\Temp\\pip-req-build-i1zp42f6'\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "jupyter-server 1.13.5 requires pywinpty<2; os_name == \"nt\", but you have pywinpty 2.0.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.3)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy>=3.2.3->pke==2.0.0) (0.4.4)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (8.0.4)\n",
      "Collecting rich>=10.11.0\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1\n",
      "  Downloading smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (1.12.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (2.0.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from nltk->pke==2.0.0) (2022.3.15)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from scikit-learn->pke==2.0.0) (2.2.0)\n",
      "Building wheels for collected packages: pke\n",
      "  Building wheel for pke (setup.py): started\n",
      "  Building wheel for pke (setup.py): finished with status 'done'\n",
      "  Created wheel for pke: filename=pke-2.0.0-py3-none-any.whl size=6161142 sha256=3ce9f69aa1d4e248daa516120f6119948394f6501b2427d7697449fca896b064\n",
      "  Stored in directory: C:\\Users\\xavmo\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-bmrn1f1u\\wheels\\d5\\46\\97\\85535b5b449f70b6a3c8d1138ce8587345876891e25bfe7954\n",
      "Successfully built pke\n",
      "Installing collected packages: typing-extensions, mdurl, pygments, pydantic-core, markdown-it-py, colorama, catalogue, annotated-types, srsly, shellingham, rich, pydantic, murmurhash, marisa-trie, cymem, wasabi, typer, smart-open, preshed, language-data, confection, cloudpathlib, blis, weasel, thinc, spacy-loggers, spacy-legacy, langcodes, spacy, pke\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Uninstalling typing-extensions-4.1.1:\n",
      "      Successfully uninstalled typing-extensions-4.1.1\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n",
      "      Successfully uninstalled Pygments-2.11.2\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 5.1.0\n",
      "    Uninstalling smart-open-5.1.0:\n",
      "      Successfully uninstalled smart-open-5.1.0\n",
      "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.18.1 colorama-0.4.6 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.10 pke-2.0.0 preshed-3.0.9 pydantic-2.7.3 pydantic-core-2.18.4 pygments-2.18.0 rich-13.7.1 shellingham-1.5.4 smart-open-7.0.4 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.4 typer-0.12.3 typing-extensions-4.12.2 wasabi-1.1.3 weasel-0.4.1\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "Requirement already satisfied: six in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=214d2e7ad772043ff6f44d866a043b5fed6cd83b3eacbeb028aecea6f11b303b\n",
      "  Stored in directory: c:\\users\\xavmo\\appdata\\local\\pip\\cache\\wheels\\d1\\c1\\d9\\7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/boudinfl/pke.git\n",
    "!pip install langdetect\n",
    "!pip install polars\n",
    "!pip install groq\n",
    "!pip install pyarrow\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pandas) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies nécessaires\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from langdetect import detect\n",
    "import json\n",
    "import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation d'un corpus : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorbonne Université à tous son corpus de publications principalement sur HAL.  \n",
    "Cependant, suite à une annonce de partenariat avec OpenAlex et allant de plus en plus sur cette bibliothèque gratuite et open source, le choix d'aller chercher dans un premier temps mon corpus dessus est donc le plus pertinant.  \n",
    "Dans un premier temps, il nous faut donc comparer la différence des mots clés utilisés dans HAL et OpenAlex via le travail de Nacef effectué sur le github suivant (https://github.com/obtic-sorbonne/keywords/blob/main/TER_Keyword_Extraction_sur_BDD_HAL.ipynb).  \n",
    "Regardons un premier temps quelles sont les métadonnées des publications sur HAL et OA en prenant pour exemple une publication de Stephane Le Crom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'docid': '122816', 'label_s': 'Marika Kapsimali, Stéphane Le Crom, Philippe Vernier. A natural history of vertebrate dopamine receptors.. Anita Sidhu, Marc Laruelle, Philippe Vernier. Dopamine Receptors and Transporters, Marcel Dekker Inc, pp.1-45, 2003. &#x27E8;hal-00122816&#x27E9;', 'uri_s': 'https://hal.science/hal-00122816'}\n",
      "docid\n",
      "label_s\n",
      "uri_s\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "hal_id = 'hal-00122816'\n",
    "url = f\"https://api.archives-ouvertes.fr/search/?q=halId_s:{hal_id}&wt=json\"\n",
    "\n",
    "response = requests.get(url)\n",
    "# Faire la requête à l'API\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    # Conversion de la réponse en JSON\n",
    "    data = json.loads(response.text)\n",
    "    # Prendre le premier document (s'il y en a plusieurs)\n",
    "    doc = data['response']['docs'][0]\n",
    "    print(doc)\n",
    "    # Vérifier s'il y a des documents dans la réponse\n",
    "    if data['response']['numFound'] > 0:\n",
    "        \n",
    "        # Afficher les noms de toutes les métadonnées\n",
    "        for key in doc.keys():\n",
    "            print(key)\n",
    "    else:\n",
    "        print(\"Aucune publication trouvée avec cet identifiant HAL.\")\n",
    "else:\n",
    "    print(f\"Erreur lors de la requête : {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['meta', 'results', 'group_by'])\n",
      "dict_keys(['id', 'doi', 'title', 'display_name', 'publication_year', 'publication_date', 'ids', 'language', 'primary_location', 'type', 'type_crossref', 'indexed_in', 'open_access', 'authorships', 'countries_distinct_count', 'institutions_distinct_count', 'corresponding_author_ids', 'corresponding_institution_ids', 'apc_list', 'apc_paid', 'fwci', 'has_fulltext', 'cited_by_count', 'cited_by_percentile_year', 'biblio', 'is_retracted', 'is_paratext', 'primary_topic', 'topics', 'keywords', 'concepts', 'mesh', 'locations_count', 'locations', 'best_oa_location', 'sustainable_development_goals', 'grants', 'datasets', 'versions', 'referenced_works_count', 'referenced_works', 'related_works', 'ngrams_url', 'abstract_inverted_index', 'cited_by_api_url', 'counts_by_year', 'updated_date', 'created_date'])\n"
     ]
    }
   ],
   "source": [
    "# ID de l'auteur\n",
    "author_id = \"a5007814380\"  \n",
    "\n",
    "# URL de l'API\n",
    "url = f\"https://api.openalex.org/works?filter=author.id:{author_id}\"\n",
    "\n",
    "# Faire la requête à l'API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Vérifier que la requête a réussi\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    # Conversion de la réponse en JSON\n",
    "    data = json.loads(response.text)\n",
    "    print(data.keys())\n",
    "    i=1\n",
    "    for work in data['results'] : \n",
    "        if i==1 :\n",
    "            print(work.keys())\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  On voit qu'il nous ait bien compliqué d'obtenir par code tous les paramètres des métadonnées d'une publication sur HAL. Je vous encourage donc à suivre\n",
    "le lien suivant afin de mieux comprendre les différents paramètres renseignés initialement (https://hal.science/help/information).  \n",
    "Nous remarquons à présent avec la seconde brique de code tous les différents paramètres renseignés par les métadonnées d'une publication d'OpenAlex. Celles-ci sont nettement plus complexes et intéressantes à utiliser que celles de HAL, et ce pour de multiples raisons :  \n",
    "- Keywords et scorings : \n",
    "- Topics : \n",
    "- Citations :  \n",
    "\n",
    "Dans un premier temps, une étude des keywords en fonction d'HAL et d'OpenAlex sera faite afin de verifier la pertinence de leurs attributions. Pour ce\n",
    "faire, nous appliquerons un raisonnement par induction. Nous sélectionnerons un plus petit corpus constitué des travaux de Stéphane Le Crom afin d'obtenir des premiers résultats rapides. Puis, nous appliquerons notre protocole sur notre corpus global pour une comparaison à plus grande échelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quel protocole utiliser ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a deux lvl pour keywords -> mots clés exacts similaires ou etymologiquement similaire (via du LLM ou scoring ??)\n",
    "#                               -> comparaison par topic de ses mots clés d'open alex (Comparaison avec les autres topics??)\n",
    "#                               -> comment calculer la similarité?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des publications de Stéphane Le Crom sur HAL et OpenAlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de publications récupérées : 64\n"
     ]
    }
   ],
   "source": [
    "auteurs = [\"Stéphane Le Crom\"]\n",
    "\n",
    "# Initialisez une liste vide pour stocker les métadonnées des publications\n",
    "publications = []\n",
    "\n",
    "# URL de base de l'API HAL\n",
    "base_url = \"https://api.archives-ouvertes.fr/search/\"\n",
    "\n",
    "# Parcourez la liste des auteurs et récupérez leurs publications\n",
    "for auteur in auteurs:\n",
    "    params = {\n",
    "        \"q\": f'authFullName_s:\"{auteur}\"',\n",
    "        \"fl\": \"authFullName_s,authIdHal_i,authIdHal_s,title_s,halId_s,producedDateY_i,doiId_s,abstract_s,uri_s,domain_s,keyword_s\",\n",
    "        \"rows\": 10000  # Augmentez le nombre de lignes si nécessaire\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        publications.extend(data[\"response\"][\"docs\"])\n",
    "    else:\n",
    "        print(f\"Erreur pour l'auteur {auteur}: {response.status_code} - {response.text}\")\n",
    "\n",
    "# Créez un DataFrame Pandas à partir des métadonnées des publications\n",
    "column_order = [\"authFullName_s\", \"authIdHal_i\", \"authIdHal_s\", \"title_s\", \"halId_s\", \"producedDateY_i\", \"doiId_s\", \"keyword_s\", \"abstract_s\", \"uri_s\", \"domain_s\"]\n",
    "\n",
    "df = pd.DataFrame(publications, columns= column_order)\n",
    "\n",
    "# Sauvegardez le DataFrame dans un fichier CSV\n",
    "df.to_csv(\"publications_hal_stephane_le_crom.csv\", index=False)\n",
    "\n",
    "# Affichez le nombre total de publications récupérées\n",
    "print(f\"Nombre total de publications récupérées : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publications restantes après nettoyage : 62\n"
     ]
    }
   ],
   "source": [
    "# Conversion les listes en chaînes dans la colonne \"title_s\"\n",
    "df[\"title_s\"] = df[\"title_s\"].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Suppression des doublons à base du titre seulement\n",
    "df = df.drop_duplicates(subset=\"title_s\")\n",
    "print(f\"Publications restantes après nettoyage : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID de l'auteur\n",
    "author_id = \"a5052835091\"\n",
    "\n",
    "# URL de l'API\n",
    "url = f\"https://api.openalex.org/works?filter=author.id:{author_id}\"\n",
    "\n",
    "# Faire la requête à l'API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Vérifier que la requête a réussi\n",
    "if response.status_code == 200:\n",
    "    # Conversion de la réponse en JSON\n",
    "    data = json.loads(response.text)\n",
    "\n",
    "    # Sélection les métadonnées précises\n",
    "    selected_data = []\n",
    "    \n",
    "    for work in data['results']:\n",
    "        \n",
    "        author_ids = []\n",
    "        if 'authorships' in work: \n",
    "            for authorship in work['authorships']:\n",
    "                # Ajoutez l'identifiant de l'auteur à la liste 'author_ids'\n",
    "                author_ids.append(authorship[\"author\"][\"id\"])\n",
    "                \n",
    "        keywords, scorings = [], [] \n",
    "        if 'keywords' in work:\n",
    "            for id in work['keywords']:\n",
    "                keywords.append(id['display_name'])\n",
    "                scorings.append(id['score'])\n",
    "                \n",
    "        selected_data.append({\n",
    "            'id' : work['id'],\n",
    "            'doi': work['doi'],\n",
    "            'title': work['title'],\n",
    "            'authors_id' : author_ids, \n",
    "            'keywords' : keywords, \n",
    "            'scorings' : scorings,\n",
    "            'referenced_works': work['referenced_works'], \n",
    "            'related_works': work['related_works'],\n",
    "            'cited_by_api_url': work['cited_by_api_url'],    \n",
    "        })\n",
    "\n",
    "    # Conversion des données sélectionnées en DataFrame\n",
    "    df2 = pd.DataFrame(selected_data)\n",
    "\n",
    "    # Exportation du DataFrame en CSV\n",
    "    df2.to_csv('publications_oa_stephane_le_crom.csv', index=False)\n",
    "else:\n",
    "    print(f\"Erreur : {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_keep_ind(groups):\n",
    "    \"\"\"\n",
    "    Function to determine if a text should be kept or not.\n",
    "\n",
    "    Input:\n",
    "    groups: list of character groups\n",
    "\n",
    "    Output:\n",
    "    0: if text should be not used\n",
    "    1: if text should be used\n",
    "    \"\"\"\n",
    "    # Groups of characters that do not perform well\n",
    "    groups_to_skip = ['HIRAGANA', 'CJK', 'KATAKANA','ARABIC', 'HANGUL', 'THAI','DEVANAGARI','BENGALI',\n",
    "                      'THAANA','GUJARATI','CYRILLIC']\n",
    "    \n",
    "    if any(x in groups_to_skip for x in groups):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def remove_non_latin_characters(text):\n",
    "    \"\"\"\n",
    "    Function to remove non-latin characters.\n",
    "\n",
    "    Input:\n",
    "    text: string of characters\n",
    "\n",
    "    Output:\n",
    "    final_char: string of characters with non-latin characters removed\n",
    "    \"\"\"\n",
    "    final_char = []\n",
    "    groups_to_skip = ['HIRAGANA', 'CJK', 'KATAKANA','ARABIC', 'HANGUL', 'THAI','DEVANAGARI','BENGALI',\n",
    "                      'THAANA','GUJARATI','CYRILLIC']\n",
    "    for char in text:\n",
    "        try:\n",
    "            script = unicodedata.name(char).split(\" \")[0]\n",
    "            if script not in groups_to_skip:\n",
    "                final_char.append(char)\n",
    "        except:\n",
    "            pass\n",
    "    return \"\".join(final_char)\n",
    "    \n",
    "def group_non_latin_characters(text):\n",
    "    \"\"\"\n",
    "    Function to group non-latin characters and return the number of latin characters.\n",
    "\n",
    "    Input:\n",
    "    text: string of characters\n",
    "\n",
    "    Output:\n",
    "    groups: list of character groups\n",
    "    latin_chars: number of latin characters\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    latin_chars = []\n",
    "    text = text.replace(\".\", \"\").replace(\" \", \"\")\n",
    "    for char in text:\n",
    "        try:\n",
    "            script = unicodedata.name(char).split(\" \")[0]\n",
    "            if script == 'LATIN':\n",
    "                latin_chars.append(script)\n",
    "            else:\n",
    "                if script not in groups:\n",
    "                    groups.append(script)\n",
    "        except:\n",
    "            if \"UNK\" not in groups:\n",
    "                groups.append(\"UNK\")\n",
    "    return groups, len(latin_chars)\n",
    "\n",
    "def check_for_non_latin_characters(text):\n",
    "    \"\"\"\n",
    "    Function to check if non-latin characters are dominant in a text.\n",
    "\n",
    "    Input:\n",
    "    text: string of characters\n",
    "\n",
    "    Output:\n",
    "    0: if text should be not used\n",
    "    1: if text should be used\n",
    "    \"\"\"\n",
    "    groups, latin_chars = group_non_latin_characters(str(text))\n",
    "    if name_to_keep_ind(groups) == 1:\n",
    "        return 1\n",
    "    elif latin_chars > 20:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def clean_title(old_title):\n",
    "    \"\"\"\n",
    "    Function to check if title should be kept and then remove non-latin characters. Also\n",
    "    removes some HTML tags from the title.\n",
    "    \n",
    "    Input:\n",
    "    old_title: string of title\n",
    "    \n",
    "    Output:\n",
    "    new_title: string of title with non-latin characters and HTML tags removed\n",
    "    \"\"\"\n",
    "    keep_title = check_for_non_latin_characters(old_title)\n",
    "    if (keep_title == 1) & isinstance(old_title, str):\n",
    "        new_title = remove_non_latin_characters(old_title)\n",
    "        if '<' in new_title:\n",
    "            new_title = new_title.replace(\"<i>\", \"\").replace(\"</i>\",\"\")\\\n",
    "                                 .replace(\"<sub>\", \"\").replace(\"</sub>\",\"\") \\\n",
    "                                 .replace(\"<sup>\", \"\").replace(\"</sup>\",\"\") \\\n",
    "                                 .replace(\"<em>\", \"\").replace(\"</em>\",\"\") \\\n",
    "                                 .replace(\"<b>\", \"\").replace(\"</b>\",\"\") \\\n",
    "                                 .replace(\"<I>\", \"\").replace(\"</I>\", \"\") \\\n",
    "                                 .replace(\"<SUB>\", \"\").replace(\"</SUB>\", \"\") \\\n",
    "                                 .replace(\"<scp>\", \"\").replace(\"</scp>\", \"\") \\\n",
    "                                 .replace(\"<font>\", \"\").replace(\"</font>\", \"\") \\\n",
    "                                 .replace(\"<inf>\",\"\").replace(\"</inf>\", \"\") \\\n",
    "                                 .replace(\"<i /> \", \"\") \\\n",
    "                                 .replace(\"<p>\", \"\").replace(\"</p>\",\"\") \\\n",
    "                                 .replace(\"<![CDATA[<B>\", \"\").replace(\"</B>]]>\", \"\") \\\n",
    "                                 .replace(\"<italic>\", \"\").replace(\"</italic>\",\"\")\\\n",
    "                                 .replace(\"<title>\", \"\").replace(\"</title>\", \"\") \\\n",
    "                                 .replace(\"<br>\", \"\").replace(\"</br>\",\"\").replace(\"<br/>\",\"\") \\\n",
    "                                 .replace(\"<B>\", \"\").replace(\"</B>\", \"\") \\\n",
    "                                 .replace(\"<em>\", \"\").replace(\"</em>\", \"\") \\\n",
    "                                 .replace(\"<BR>\", \"\").replace(\"</BR>\", \"\") \\\n",
    "                                 .replace(\"<title>\", \"\").replace(\"</title>\", \"\") \\\n",
    "                                 .replace(\"<strong>\", \"\").replace(\"</strong>\", \"\") \\\n",
    "                                 .replace(\"<formula>\", \"\").replace(\"</formula>\", \"\") \\\n",
    "                                 .replace(\"<roman>\", \"\").replace(\"</roman>\", \"\") \\\n",
    "                                 .replace(\"<SUP>\", \"\").replace(\"</SUP>\", \"\") \\\n",
    "                                 .replace(\"<SSUP>\", \"\").replace(\"</SSUP>\", \"\") \\\n",
    "                                 .replace(\"<sc>\", \"\").replace(\"</sc>\", \"\") \\\n",
    "                                 .replace(\"<subtitle>\", \"\").replace(\"</subtitle>\", \"\") \\\n",
    "                                 .replace(\"<emph/>\", \"\").replace(\"<emph>\", \"\").replace(\"</emph>\", \"\") \\\n",
    "                                 .replace(\"\"\"<p class=\"Body\">\"\"\", \"\") \\\n",
    "                                 .replace(\"<TITLE>\", \"\").replace(\"</TITLE>\", \"\") \\\n",
    "                                 .replace(\"<sub />\", \"\").replace(\"<sub/>\", \"\") \\\n",
    "                                 .replace(\"<mi>\", \"\").replace(\"</mi>\", \"\") \\\n",
    "                                 .replace(\"<bold>\", \"\").replace(\"</bold>\", \"\") \\\n",
    "                                 .replace(\"<mtext>\", \"\").replace(\"</mtext>\", \"\") \\\n",
    "                                 .replace(\"<msub>\", \"\").replace(\"</msub>\", \"\") \\\n",
    "                                 .replace(\"<mrow>\", \"\").replace(\"</mrow>\", \"\") \\\n",
    "                                 .replace(\"</mfenced>\", \"\").replace(\"</math>\", \"\")\n",
    "\n",
    "            if '<mml' in new_title:\n",
    "                all_parts = [x for y in [i.split(\"mml:math>\") for i in new_title.split(\"<mml:math\")] for x in y if x]\n",
    "                final_parts = []\n",
    "                for part in all_parts:\n",
    "                    if re.search(r\"\\>[$%#!^*\\w.,/()+-]*\\<\", part):\n",
    "                        pull_out = re.findall(r\"\\>[$%#!^*\\w.,/()+-]*\\<\", part)\n",
    "                        final_pieces = []\n",
    "                        for piece in pull_out:\n",
    "                            final_pieces.append(piece.replace(\">\", \"\").replace(\"<\", \"\"))\n",
    "                        \n",
    "                        final_parts.append(\" \"+ \"\".join(final_pieces) + \" \")\n",
    "                    else:\n",
    "                        final_parts.append(part)\n",
    "                \n",
    "                new_title = \"\".join(final_parts).strip()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if '<xref' in new_title:\n",
    "                new_title = re.sub(r\"\\<xref[^/]*\\/xref\\>\", \"\", new_title)\n",
    "\n",
    "            if '<inline-formula' in new_title:\n",
    "                new_title = re.sub(r\"\\<inline-formula[^/]*\\/inline-formula\\>\", \"\", new_title)\n",
    "\n",
    "            if '<title' in new_title:\n",
    "                new_title = re.sub(r\"\\<title[^/]*\\/title\\>\", \"\", new_title)\n",
    "\n",
    "            if '<p class=' in new_title:\n",
    "                new_title = re.sub(r\"\\<p class=[^>]*\\>\", \"\", new_title)\n",
    "            \n",
    "            if '<span class=' in new_title:\n",
    "                new_title = re.sub(r\"\\<span class=[^>]*\\>\", \"\", new_title)\n",
    "\n",
    "            if 'mfenced open' in new_title:\n",
    "                new_title = re.sub(r\"\\<mfenced open=[^>]*\\>\", \"\", new_title)\n",
    "            \n",
    "            if 'math xmlns' in new_title:\n",
    "                new_title = re.sub(r\"\\<math xmlns=[^>]*\\>\", \"\", new_title)\n",
    "\n",
    "        if '<' in new_title:\n",
    "            new_title = new_title.replace(\">i<\", \"\").replace(\">/i<\", \"\") \\\n",
    "                                 .replace(\">b<\", \"\").replace(\">/b<\", \"\") \\\n",
    "                                 .replace(\"<inline-formula>\", \"\").replace(\"</inline-formula>\",\"\")\n",
    "        if new_title.isupper():\n",
    "            new_title = new_title.title()\n",
    "        \n",
    "        return new_title\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def liaison_second(df1_hal, df2_oa):\n",
    "    # On clean les titres de la df d'OpenAlex\n",
    "    for i, title in enumerate(df2_oa['title']):\n",
    "        new_title = clean_title(title)  # clean_title : la fonction de nettoyage appropriée\n",
    "        df2_oa.loc[i, 'title'] = new_title\n",
    "\n",
    "    # On convertit les titres d'oa en liste\n",
    "    list_oa_title = df2_oa['title'].tolist()\n",
    "\n",
    "    # On supprime toutes les publications qui ne sont pas en commun\n",
    "    indexes_to_drop = []\n",
    "    for i, title in enumerate(df1_hal['title_s']):\n",
    "        if title not in list_oa_title:\n",
    "            indexes_to_drop.append(i)\n",
    "\n",
    "    df1_hal.drop(indexes_to_drop, inplace=True)\n",
    "    \n",
    "def clean_doi_oa(old_doi):\n",
    "    return old_doi.replace(\"https://doi.org/\", \"\")\n",
    "\n",
    "def liaison(df1_hal, df2_oa):\n",
    "    # On clean les titres de la df d'OpenAlex\n",
    "    for i, doi in enumerate(df2_oa['doi']):\n",
    "        new_doi = clean_doi_oa(doi)\n",
    "        df2_oa.loc[i, 'doi'] = new_doi\n",
    "\n",
    "    # On convertit les titres d'oa en liste\n",
    "    list_hal_title = df1_hal['doiId_s'].tolist()\n",
    "    df2_change = pd.DataFrame()\n",
    "\n",
    "    # On supprime toutes les publications qui ne sont pas en commun\n",
    "    indexes_to_drop = []\n",
    "    \n",
    "    for i, doi in enumerate(df2_oa['doi']):\n",
    "        if doi in list_hal_title:\n",
    "            #df2_change = df2_change.append(df1_hal.iloc[i])\n",
    "            df2_change = pd.concat([df2_change, df1_hal.iloc[[i]]])\n",
    "        else : \n",
    "            indexes_to_drop.append(i)\n",
    "            \n",
    "    df2_oa.drop(indexes_to_drop, inplace=True)\n",
    "    df2_change.to_csv('pub_hal_liees.csv', index=False)\n",
    "    \n",
    "    return df2_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "df2_change = liaison(df,df2)\n",
    "print(len(df2_change))\n",
    "print(len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "def extract_keywords_groq(text, models):\n",
    "    prompt = f\"As a keyword extraction master, your only mission here is to extract only the most relevent keywords that are present in the text. Put the list of keywords between brackets, comma-seperated.DO NOT write something else than the keywords you're supposed to extract from the text. Skip the preamble and provide only the best keywords (present in the text). The text:\\n{text} VERY IMPORTANT : Put the list of keywords between brackets, comma-seperated like this ['keyword1', 'keyword2, 'keyword3' etc...] NO TICKS only what ive said\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"[\"\n",
    "            }\n",
    "        ],\n",
    "        model=models,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    keyword_list = [keyword.strip('\"') for keyword in response.strip(\"[]\").split(', ')]\n",
    "\n",
    "\n",
    "    return keyword_list\n",
    "\n",
    "def extract_groq(df, list_models, use_title = False):\n",
    "    \n",
    "    for model in list_models :\n",
    "\n",
    "        list_kw = []\n",
    "\n",
    "        for row in trange(len(df)):\n",
    "            text = df.loc[row,'abstract_s']\n",
    "            if pd.isnull(text):\n",
    "                text = ''\n",
    "            else:\n",
    "                text = str(text)\n",
    "                \n",
    "            if use_title:\n",
    "                title = df.loc[row, 'title_s']\n",
    "                if not pd.isnull(title):\n",
    "                    text += str(title)\n",
    "                    \n",
    "            kw = extract_keywords_groq(text, model)\n",
    "            list_kw.append(kw)\n",
    "            \n",
    "        list_of_dicts = [{'list of kw': keywords} for keywords in list_kw]\n",
    "        df_kw = pl.DataFrame(list_of_dicts)\n",
    "\n",
    "        df[model] = df_kw['list of kw']\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:11<00:00,  2.14it/s]\n",
      "100%|██████████| 25/25 [00:43<00:00,  1.74s/it]\n",
      "100%|██████████| 25/25 [00:10<00:00,  2.32it/s]\n",
      "100%|██████████| 25/25 [01:34<00:00,  3.79s/it]\n"
     ]
    }
   ],
   "source": [
    "models = ['llama3-8b-8192',\n",
    "          #'gemma-7b-it',\n",
    "          \"llama3-70b-8192\"]\n",
    "\n",
    "df = pd.read_csv('pub_hal_liees.csv')\n",
    "\n",
    "df_llm = extract_groq(df, models)\n",
    "df_llm_title = extract_groq(df_llm, models, use_title = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llm_title.to_csv('pub_hal_liees_keywords.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authFullName_s</th>\n",
       "      <th>authIdHal_i</th>\n",
       "      <th>authIdHal_s</th>\n",
       "      <th>title_s</th>\n",
       "      <th>halId_s</th>\n",
       "      <th>producedDateY_i</th>\n",
       "      <th>doiId_s</th>\n",
       "      <th>keyword_s</th>\n",
       "      <th>abstract_s</th>\n",
       "      <th>uri_s</th>\n",
       "      <th>domain_s</th>\n",
       "      <th>llama3-8b-8192</th>\n",
       "      <th>llama3-70b-8192</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Thomas Portnoy', 'Antoine Margeot', 'Verena ...</td>\n",
       "      <td>[919327, 937936, 13760, 919328, 919335, 919336]</td>\n",
       "      <td>['stephane-le-crom']</td>\n",
       "      <td>Differential regulation of the cellulase trans...</td>\n",
       "      <td>hal-02879336</td>\n",
       "      <td>2011</td>\n",
       "      <td>10.1128/EC.00208-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Due to its capacity to produce large amounts...</td>\n",
       "      <td>https://hal.sorbonne-universite.fr/hal-02879336</td>\n",
       "      <td>['0.sdv', '1.sdv.bbm', '2.sdv.bbm.gtp', '0.sdv...</td>\n",
       "      <td>[Trichoderma reesei, cellulases, biofuel, lign...</td>\n",
       "      <td>[Trichoderma reesei, cellulase, biofuel, ligno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Fanny Coulpier', 'Stéphane Le Crom', 'Gérald...</td>\n",
       "      <td>[13760]</td>\n",
       "      <td>['stephane-le-crom']</td>\n",
       "      <td>Novel features of boundary cap cells revealed ...</td>\n",
       "      <td>hal-02879350</td>\n",
       "      <td>2009</td>\n",
       "      <td>10.1002/glia.20862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Neural crest (NC) cells are a multipotent, h...</td>\n",
       "      <td>https://hal.sorbonne-universite.fr/hal-02879350</td>\n",
       "      <td>['0.sdv', '1.sdv.bbm', '2.sdv.bbm.gtp', '0.sdv...</td>\n",
       "      <td>[Neural crest, boundary cap, Schwann cells, pe...</td>\n",
       "      <td>[Neural crest, Peripheral nervous system, Glia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Nathalie Lehmann', 'Sandrine Perrin', 'Clair...</td>\n",
       "      <td>[1148850, 174545, 13760, 748529, 185045]</td>\n",
       "      <td>['charlotte-berthelier', 'celine-hernandez', '...</td>\n",
       "      <td>Eoulsan 2: an efficient workflow manager for r...</td>\n",
       "      <td>hal-03784195</td>\n",
       "      <td>2021</td>\n",
       "      <td>10.1101/2021.10.13.464219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['A bstract Motivation Core sequencing facilit...</td>\n",
       "      <td>https://hal.science/hal-03784195</td>\n",
       "      <td>['0.sdv', '1.sdv.bibs']</td>\n",
       "      <td>[Eoulsan, workflow engine, sequencing data, tr...</td>\n",
       "      <td>[Eoulsan, workflow, sequencing, RNA-seq, trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Andrea Sartirana', 'Laurent Jourdren', 'Paul...</td>\n",
       "      <td>[950950, 185045, 950951, 1228770, 741166, 13760]</td>\n",
       "      <td>['laurent-jourdren', 'david-chamont', 'phbusso...</td>\n",
       "      <td>Eoulsan : analyse du séquençage à haut débit d...</td>\n",
       "      <td>hal-00927507</td>\n",
       "      <td>2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Eoulsan : analyse du séquençage à haut débit...</td>\n",
       "      <td>https://hal.science/hal-00927507</td>\n",
       "      <td>['0.info', '1.info.info-dc']</td>\n",
       "      <td>[Eoulsan, séquençage, débit, cloud, grille]</td>\n",
       "      <td>[Eoulsan, analyse, séquençage, haut débit, clo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['Sophie Lemoine', 'Florence Combes', 'Nicolas...</td>\n",
       "      <td>[908599, 918596, 13760]</td>\n",
       "      <td>['lemoine-sophie', 'stephane-le-crom']</td>\n",
       "      <td>Goulphar: rapid access and expertise for stand...</td>\n",
       "      <td>inserm-00122139</td>\n",
       "      <td>2006</td>\n",
       "      <td>10.1186/1471-2105-7-467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['BACKGROUND: Raw data normalization is a crit...</td>\n",
       "      <td>https://inserm.hal.science/inserm-00122139</td>\n",
       "      <td>['0.sdv', '1.sdv.bibs']</td>\n",
       "      <td>[raw data normalization, microarray data, R/Bi...</td>\n",
       "      <td>[microarray, normalization, R, BioConductor, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      authFullName_s  \\\n",
       "0  ['Thomas Portnoy', 'Antoine Margeot', 'Verena ...   \n",
       "1  ['Fanny Coulpier', 'Stéphane Le Crom', 'Gérald...   \n",
       "2  ['Nathalie Lehmann', 'Sandrine Perrin', 'Clair...   \n",
       "3  ['Andrea Sartirana', 'Laurent Jourdren', 'Paul...   \n",
       "4  ['Sophie Lemoine', 'Florence Combes', 'Nicolas...   \n",
       "\n",
       "                                        authIdHal_i  \\\n",
       "0   [919327, 937936, 13760, 919328, 919335, 919336]   \n",
       "1                                           [13760]   \n",
       "2          [1148850, 174545, 13760, 748529, 185045]   \n",
       "3  [950950, 185045, 950951, 1228770, 741166, 13760]   \n",
       "4                           [908599, 918596, 13760]   \n",
       "\n",
       "                                         authIdHal_s  \\\n",
       "0                               ['stephane-le-crom']   \n",
       "1                               ['stephane-le-crom']   \n",
       "2  ['charlotte-berthelier', 'celine-hernandez', '...   \n",
       "3  ['laurent-jourdren', 'david-chamont', 'phbusso...   \n",
       "4             ['lemoine-sophie', 'stephane-le-crom']   \n",
       "\n",
       "                                             title_s          halId_s  \\\n",
       "0  Differential regulation of the cellulase trans...     hal-02879336   \n",
       "1  Novel features of boundary cap cells revealed ...     hal-02879350   \n",
       "2  Eoulsan 2: an efficient workflow manager for r...     hal-03784195   \n",
       "3  Eoulsan : analyse du séquençage à haut débit d...     hal-00927507   \n",
       "4  Goulphar: rapid access and expertise for stand...  inserm-00122139   \n",
       "\n",
       "   producedDateY_i                    doiId_s keyword_s  \\\n",
       "0             2011        10.1128/EC.00208-10       NaN   \n",
       "1             2009         10.1002/glia.20862       NaN   \n",
       "2             2021  10.1101/2021.10.13.464219       NaN   \n",
       "3             2013                        NaN       NaN   \n",
       "4             2006    10.1186/1471-2105-7-467       NaN   \n",
       "\n",
       "                                          abstract_s  \\\n",
       "0  ['Due to its capacity to produce large amounts...   \n",
       "1  ['Neural crest (NC) cells are a multipotent, h...   \n",
       "2  ['A bstract Motivation Core sequencing facilit...   \n",
       "3  ['Eoulsan : analyse du séquençage à haut débit...   \n",
       "4  ['BACKGROUND: Raw data normalization is a crit...   \n",
       "\n",
       "                                             uri_s  \\\n",
       "0  https://hal.sorbonne-universite.fr/hal-02879336   \n",
       "1  https://hal.sorbonne-universite.fr/hal-02879350   \n",
       "2                 https://hal.science/hal-03784195   \n",
       "3                 https://hal.science/hal-00927507   \n",
       "4       https://inserm.hal.science/inserm-00122139   \n",
       "\n",
       "                                            domain_s  \\\n",
       "0  ['0.sdv', '1.sdv.bbm', '2.sdv.bbm.gtp', '0.sdv...   \n",
       "1  ['0.sdv', '1.sdv.bbm', '2.sdv.bbm.gtp', '0.sdv...   \n",
       "2                            ['0.sdv', '1.sdv.bibs']   \n",
       "3                       ['0.info', '1.info.info-dc']   \n",
       "4                            ['0.sdv', '1.sdv.bibs']   \n",
       "\n",
       "                                      llama3-8b-8192  \\\n",
       "0  [Trichoderma reesei, cellulases, biofuel, lign...   \n",
       "1  [Neural crest, boundary cap, Schwann cells, pe...   \n",
       "2  [Eoulsan, workflow engine, sequencing data, tr...   \n",
       "3        [Eoulsan, séquençage, débit, cloud, grille]   \n",
       "4  [raw data normalization, microarray data, R/Bi...   \n",
       "\n",
       "                                     llama3-70b-8192  \n",
       "0  [Trichoderma reesei, cellulase, biofuel, ligno...  \n",
       "1  [Neural crest, Peripheral nervous system, Glia...  \n",
       "2  [Eoulsan, workflow, sequencing, RNA-seq, trans...  \n",
       "3  [Eoulsan, analyse, séquençage, haut débit, clo...  \n",
       "4  [microarray, normalization, R, BioConductor, d...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_llm_title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//////////////////////////////////////////////////////////////////////////////////////////////////////////////////  \n",
    "/////////////////////////////////////////////////////////////////////////////////////////////////////////////////  \n",
    "////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de publications récupérées : 12388\n"
     ]
    }
   ],
   "source": [
    "# Script de Waly modifié.\n",
    "# Liste des noms d'auteurs que vous souhaitez rechercher\n",
    "auteurs = [\n",
    "    \"Anne-Virginie Salsac\",\n",
    "    \"Dan Istrate\",\n",
    "    \"Eric Leclerc\",\n",
    "    \"Julien \\\"Le Duigou\\\"\",\n",
    "    \"Marie-Christine \\\"Ho Ba Tho\\\"\",\n",
    "    \"Sofiane Boudaoud\",\n",
    "    \"Glenn Roe\",\n",
    "    \"Motasem ALRAHABI\",\n",
    "    \"Arnaud Latil\",\n",
    "    \"Christian NERI\",\n",
    "    \"Clément Mabi\",\n",
    "    \"David Flacher\",\n",
    "    \"M. Shawky\",\n",
    "    \"Serge Bouchardon\",\n",
    "    \"Harry Sokol\",\n",
    "    \"Bérangère Bihan-Avalle\",\n",
    "    \"Caroline Marti\",\n",
    "    \"Laurent Petit\",\n",
    "    \"Pierre-Carl Langlais\",\n",
    "    \"David Klatzmann\",\n",
    "    \"Raphael Gavazzi\",\n",
    "    \"Benjamin Wandelt\",\n",
    "    \"christophe pichon\",\n",
    "    \"Guilhem Lavaux\",\n",
    "    \"Henry Joy McCracken\",\n",
    "    \"Kumiko Kotera\",\n",
    "    \"Yohan Dubois\",\n",
    "    \"A. Marco Saitta\",\n",
    "    \"Dirk Stratmann\",\n",
    "    \"Guillaume Ferlat\",\n",
    "    \"Slavica Jonic\",\n",
    "    \"Alex Chin\",\n",
    "    \"Fabrice Carrat\",\n",
    "    \"Pierre-Yves Boëlle\",\n",
    "    \"Renaud Piarroux\",\n",
    "    \"Christophe Guillotel-Nothmann\",\n",
    "    \"Jean-Marc Chouvel\",\n",
    "    \"Nicolas Obin\",\n",
    "    \"Philippe Esling\",\n",
    "    \"Alexandre Coninx\",\n",
    "    \"Baptiste Caramiaux\",\n",
    "    \"Benjamin Piwowarski\",\n",
    "    \"Catherine Achard\",\n",
    "    \"Catherine Pelachaud\",\n",
    "    \"Gilles Bailly\",\n",
    "    \"Jérôme Szewczyk\",\n",
    "    \"Kevin Bailly\",\n",
    "    \"Laure Soulier\",\n",
    "    \"Marie-Aude Vitrani\",\n",
    "    \"Matthieu Cord\",\n",
    "    \"Mehdi Khamassi\",\n",
    "    \"Mohamed CHETOUANI\",\n",
    "    \"Nathanaël Jarrassé\",\n",
    "    \"Nicolas Bredeche\",\n",
    "    \"Nicolas Perrin-Gilbert\",\n",
    "    \"Nicolas Thome\",\n",
    "    \"Olivier Schwander\",\n",
    "    \"Olivier Sigaud\",\n",
    "    \"Pascal Morin\",\n",
    "    \"Pierre Bessière\",\n",
    "    \"Sinan Haliyo\",\n",
    "    \"Stéphane Doncieux\",\n",
    "    \"Alessandra Carbone\",\n",
    "    \"Elodie Laine\",\n",
    "    \"Martin Weigt\",\n",
    "    \"Benoit Semelin\",\n",
    "    \"Emeric Bron\",\n",
    "    \"Emmanuel Bertin\",\n",
    "    \"Françoise Combes\",\n",
    "    \"Maryvonne Gerin\",\n",
    "    \"Philippe Salomé\",\n",
    "    \"Baptiste Cecconi\",\n",
    "    \"Philippe Zarka\",\n",
    "    \"Ferdinand Dhombres\",\n",
    "    \"Jean Charlet\",\n",
    "    \"Xavier Tannier\",\n",
    "    \"Amal \\\"El Fallah Seghrouchni\\\"\",\n",
    "    \"Andrea Pinna\",\n",
    "    \"Antoine Miné\",\n",
    "    \"Béatrice Bérard\",\n",
    "    \"Bertrand Granado\",\n",
    "    \"Bruno Escoffier\",\n",
    "    \"Carola Doerr\",\n",
    "    \"Christoph Dürr\",\n",
    "    \"Christophe Denis\",\n",
    "    \"Christophe Marsala\",\n",
    "    \"Colette Faucher\",\n",
    "    \"Emmanuel HYON\",\n",
    "    \"Emmanuelle Encrenaz-Tiphène\",\n",
    "    \"Evripidis Bampis\",\n",
    "    \"Fanny Pascual\",\n",
    "    \"Haralampos Stratigopoulos\",\n",
    "    \"Jean-Daniel Kant\",\n",
    "    \"Jean-Gabriel Ganascia\",\n",
    "    \"Jean-Noël Vittaut\",\n",
    "    \"Lionel Tabourier\",\n",
    "    \"Maria Potop-Butucaru\",\n",
    "    \"Matthieu Latapy\",\n",
    "    \"Nicolas MAUDET\",\n",
    "    \"Olivier Spanjaard\",\n",
    "    \"Patrice Perny\",\n",
    "    \"Patrick Gallinari\",\n",
    "    \"Sébastien Tixeuil\",\n",
    "    \"Spyros Angelopoulos\",\n",
    "    \"Stéphane Gançarski\",\n",
    "    \"Vanda Luengo\",\n",
    "    \"Vincent Guigue\",\n",
    "    \"Bruno Despres\",\n",
    "    \"Frédéric Nataf\",\n",
    "    \"Julien Brajard\",\n",
    "    \"Sylvie Thiria\",\n",
    "    \"Catherine Matias\",\n",
    "    \"Charlotte Dion-Blanc\",\n",
    "    \"Claire Boyer\",\n",
    "    \"Gérard Biau\",\n",
    "    \"Gregory Nuel\",\n",
    "    \"Idris Kharroubi\",\n",
    "    \"Maud Thomas\",\n",
    "    \"Maxime Sangnier\",\n",
    "    \"Olivier Lopez\",\n",
    "    \"Sylvain Le-Corff\",\n",
    "    \"Tabea Rebafka\",\n",
    "    \"Benjamin Fuks\",\n",
    "    \"Stéphane Mottelet\",\n",
    "    \"Tien-Tuan Dao\",\n",
    "    \"julien mozziconacci\",\n",
    "    \"Nicolas Aunai\",\n",
    "    \"Thierry Dufour\",\n",
    "    \"Abdenour Hadid\",\n",
    "    \"Benjamin Quost\",\n",
    "    \"Bruno Toupance\",\n",
    "    \"Dominique Lenne\",\n",
    "    \"Evelyne Heyer\",\n",
    "    \"Franz Manni\",\n",
    "    \"Grace Younes\",\n",
    "    \"Lama Tarsissi\",\n",
    "    \"Marie-Hélène (Mylène) Masson\",\n",
    "    \"Marie-Hélène Abel\",\n",
    "    \"Nathalie Martial-Braz\",\n",
    "    \"Nicolas Patin\",\n",
    "    \"Philippe Bonnifait\",\n",
    "    \"Philippe Boulanger\",\n",
    "    \"Philippe Trigano\",\n",
    "    \"Raed Abu Zitar\",\n",
    "    \"Samuel F. Feng\",\n",
    "    \"Sébastien Destercke\",\n",
    "    \"Tanujit Chakraborty\",\n",
    "    \"Yves Grandvalet\",\n",
    "    \"Zoheir ABOURA\"\n",
    "]\n",
    "\n",
    "# Initialisez une liste vide pour stocker les métadonnées des publications\n",
    "publications = []\n",
    "\n",
    "# URL de base de l'API HAL\n",
    "base_url = \"https://api.archives-ouvertes.fr/search/\"\n",
    "\n",
    "# Parcourez la liste des auteurs et récupérez leurs publications\n",
    "for auteur in auteurs:\n",
    "    params = {\n",
    "        \"q\": f'authFullName_s:\"{auteur}\"',\n",
    "        \"fl\": \"authFullName_s,authIdHal_i,authIdHal_s,title_s,halId_s,producedDateY_i,doiId_s,abstract_s,uri_s,domain_s,keyword_s\",\n",
    "        \"rows\": 10000  # Augmentez le nombre de lignes si nécessaire\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        publications.extend(data[\"response\"][\"docs\"])\n",
    "    else:\n",
    "        print(f\"Erreur pour l'auteur {auteur}: {response.status_code} - {response.text}\")\n",
    "\n",
    "# Créez un DataFrame Pandas à partir des métadonnées des publications\n",
    "column_order = [\"authFullName_s\", \"authIdHal_i\", \"authIdHal_s\", \"title_s\", \"halId_s\", \"producedDateY_i\", \"doiId_s\", \"keyword_s\", \"abstract_s\", \"uri_s\", \"domain_s\"]\n",
    "\n",
    "df = pd.DataFrame(publications, columns= column_order)\n",
    "\n",
    "# Sauvegardez le DataFrame dans un fichier CSV\n",
    "#df.to_csv(\"publications_hal_scai_complet.csv\", index=False)\n",
    "\n",
    "# Affichez le nombre total de publications récupérées\n",
    "print(f\"Nombre total de publications récupérées : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publications restantes après nettoyage : 4206\n"
     ]
    }
   ],
   "source": [
    "# Conversion les listes en chaînes dans la colonne \"title_s\"\n",
    "df[\"title_s\"] = df[\"title_s\"].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Suppression des doublons\n",
    "df = df.drop_duplicates(subset=\"title_s\")\n",
    "dfp = df.dropna(subset=[\"abstract_s\", \"keyword_s\"])\n",
    "print(f\"Publications restantes après nettoyage : {len(dfp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Détection de la langue terminée \n",
      "\n",
      "Nombre d'articles anglais :\n",
      "3547\n",
      "Nombre d'articles francais :\n",
      "654\n",
      "Nombre d'articles allemand :\n",
      "2\n",
      "Index des articles avec langue inconnue\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "dfp = dfp.copy()\n",
    "dfp['language'] = 'unknown'\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "def lang(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i,'language'] = detect_language(str(df.loc[i,'abstract_s']))\n",
    "    print(\"Détection de la langue terminée \\n\")\n",
    "    return df\n",
    "\n",
    "dfp = lang(dfp)\n",
    "\n",
    "def counter(df):\n",
    "    counts = df['language'].value_counts()\n",
    "    print(\"Nombre d'articles anglais :\")\n",
    "    print(counts.get('en', 0))\n",
    "\n",
    "    print(\"Nombre d'articles francais :\")\n",
    "    print(counts.get('fr', 0))\n",
    "\n",
    "    print(\"Nombre d'articles allemand :\")\n",
    "    print(counts.get('de', 0))\n",
    "\n",
    "    print(\"Index des articles avec langue inconnue\")\n",
    "    print(df[df['language'] == 'UNKNOWN'].index.tolist())\n",
    "    return\n",
    "\n",
    "counter(dfp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID de l'auteur\n",
    "author_id = \"a5007814380\"  \n",
    "\n",
    "# URL de l'API\n",
    "url = f\"https://api.openalex.org/works?filter=author.id:{author_id}\"\n",
    "\n",
    "# Faire la requête à l'API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Vérifier que la requête a réussi\n",
    "if response.status_code == 200:\n",
    "    # Conversion de la réponse en JSON\n",
    "    data = json.loads(response.text)\n",
    "\n",
    "    # Sélection les métadonnées précises\n",
    "    selected_data = []\n",
    "    \n",
    "    for work in data['results']:\n",
    "        \n",
    "        author_ids = []\n",
    "        if 'authorships' in work: \n",
    "            for authorship in work['authorships']:\n",
    "                # Ajoutez l'identifiant de l'auteur à la liste 'author_ids'\n",
    "                author_ids.append(authorship[\"author\"][\"id\"])\n",
    "                \n",
    "        selected_data.append({\n",
    "            'id' : work['id'],\n",
    "            'title': work['title'],\n",
    "            'authors_id' : author_ids, \n",
    "            'referenced_works': work['referenced_works'], \n",
    "            'related_works': work['related_works'], \n",
    "            'cited_by_api_url': work['cited_by_api_url'],    \n",
    "        })\n",
    "\n",
    "    # Conversion des données sélectionnées en DataFrame\n",
    "    df = pd.DataFrame(selected_data)\n",
    "\n",
    "    # Exportation du DataFrame en CSV\n",
    "    df.to_csv('meta_pub_oa.csv', index=False)\n",
    "else:\n",
    "    print(f\"Erreur : {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auteurs = [\n",
    "    \"a5063079937\",\n",
    "    \"a5064456106\",\n",
    "    \"a5075692514\",\n",
    "    \"a5076657262\",\n",
    "    \"a5060263799\",\n",
    "    \"a5059667793\",\n",
    "    \"a5010298238\",\n",
    "    \"a5009962581\",\n",
    "    \"a5019206372\",\n",
    "    \"a5026117716\",\n",
    "    \"Clément Mabi\",\n",
    "    \"a5045653236\", #David Flacher\n",
    "    # pas là \"M. Shawky\",\n",
    "    \"a5054128778\", #Serge Bouchardon \n",
    "    \"a5083701333\",\n",
    "    #Bérangère Bihan-Avalle\n",
    "    #Caroline Marti\n",
    "    \"a5043441322\", #Laurent Petit\n",
    "    \"a5065665642\", # ou  Pierre-Carl Langlais a5090355875 \n",
    "    \"a5044456322\",\n",
    "    \"a5072965626\",\n",
    "    \"a5050309898\",\n",
    "    \"a5022276793\",\n",
    "    \"a5035893506\", #Guilhem Lavaux, y en a 2 donc un dechet\n",
    "    #\"Henry Joy McCracken\", chelou\n",
    "    \"a5051397820\",\n",
    "    \"a5075521248\", # Yohan Dubois en x2\n",
    "    \"a5048168479\",\n",
    "    \"a5008261497\",\n",
    "    \"a5006372418\",\n",
    "    # existe pas \"Slavica Jonic\",\n",
    "    \"a5032837587\",\n",
    "    \"a5082959305\",\n",
    "    \"a5000035181\", # y en a 2?? \"Pierre-Yves Boëlle\",\n",
    "    \"a5008914363\", # y en a 2 \"Renaud Piarroux\",\n",
    "    \"a5012443434\",\n",
    "    \"a5086118723\", # Jean marc chouvel y en a 3\n",
    "    \"a5042745853\",\n",
    "    \"a5085170922\",\n",
    "    \"a5073978648\",\n",
    "    \"a5060825472\",\n",
    "    \"a5009145141\", # 2 differents \"Benjamin Piwowarski\",\n",
    "    \"a5005922530\", # 2 a cause du nom \"Catherine Achard\",\n",
    "    \"a5079026902\", # 2 a cause du nom \"Catherine Pelachaud\",\n",
    "    \"a5019033794\", # plusieurs \"Gilles Bailly\",\n",
    "    \"a5084974660\",\n",
    "    \"a5015493311\",\n",
    "    \"a5000942708\", # 2 differents \"Laure Soulier\",\n",
    "    \"a5024969402\",\n",
    "    \"a5022871131\", # 2 differents Matthieu Cord\n",
    "    \"a5016381589\",\n",
    "    \"a5049398785\",\n",
    "    \"a5057665161\", # 2 différents \"Nathanaël Jarrassé\",\n",
    "    \"a5008211251\",\n",
    "    \"a5047587289\",\n",
    "    \"a5017490804\",\n",
    "    \"a5018812634\",\n",
    "    \"a5042850624\",\n",
    "    \"a5020385150\", # 2 differents importants \"Pascal Morin\",\n",
    "    \"a5053141872\", # 2 differents \"Pierre Bessière\",\n",
    "    \"a5016542830\", # 2 differents \"Sinan Haliyo\",\n",
    "    \"a5003629424\",\n",
    "    \"a5038174177\", # 2 differents importants  \"Alessandra Carbone\" \n",
    "    \"a5076453514\", # 2 differents \"Elodie Laine\",\n",
    "    \"a5035771024\",\n",
    "    \"a5049132515\",\n",
    "    \"a5007007631\",\n",
    "    \"a5089708177\", # doute \"Emmanuel Bertin\",\n",
    "    \"a5064812526\",\n",
    "    \"a5008102443\", # 2 differents \"Maryvonne Gerin\",\n",
    "    \"a5061694871\",\n",
    "    \"a5032663932\",\n",
    "    \"a5074319502\", # 2 differents \" Philippe Zarka\",\n",
    "    \"a5005420349\",\n",
    "    \"a5055383240\",\n",
    "    \"a5056834851\", # 2 differents \" Xavier Tannier\",\n",
    "    \"a5044546919\", # plusieurs differents importants  \" Amal El Fallah-Seghrouchni \" \n",
    "    \"a5019342840\",\n",
    "    \"a5069148908\",\n",
    "    \"a5008504744\",\n",
    "    \"a5057674250\",\n",
    "    \"a5020410268\",\n",
    "    \"a5040561209\",\n",
    "    \"a5029270613\",\n",
    "    \"a5003029415\",\n",
    "    \"a5076504951\", # 2 differents \" Christophe Marsala\",\n",
    "    \"a5008947610\",\n",
    "    \"a5062847773\", # 2 differents \" Emmanuel Hyon \"\n",
    "    \"a5040506059\",\n",
    "    \"a5063171222\", # 2 differents \"Evripidis Bampis \"\n",
    "    \"a5031467845\",\n",
    "    \"a5091734149\",\n",
    "    \"a5020645950\", # 2 differents \" Jean-Daniel Kant \"\n",
    "    \"a5052738299\", # 2 differents \"Jean‐Gabriel Ganascia \"\n",
    "    \"a5009688030\",\n",
    "    \"a5056981393\",\n",
    "    \"a5080217489\", # 2 differents Maria Potop-Butucaru \n",
    "    \"a5031952531\", # 2 differents Matthieu Latapy  \n",
    "    \"a5000925369\",\n",
    "    \"a5012670875\",\n",
    "    \"a5025612115\",\n",
    "    \"a5086752907\",\n",
    "    \"a5073883755\", # 2 differents Sébastien Tixeuil \n",
    "    \"a5063338404\", # 2 differents Spyros Angelopoulos \n",
    "    \"a5049035512\",\n",
    "    \"a5073711335\",\n",
    "    \"a5044389669\",\n",
    "    \"a5090273130\",\n",
    "    \"a5004798802\",\n",
    "    \"a5073300655\", # 3 differents Julien Brajard\n",
    "    \"a5042022387\", # 2 differents Sylvie Thiria \n",
    "    \"a5082187550\",\n",
    "    \"a5011096724\",\n",
    "    \"a5010301554\",\n",
    "    \"a5007814380\",\n",
    "    \"a5087944743\",\n",
    "    \"a5051885232\",# 2 differents Idris Kharroubi \n",
    "    \"a5076190399\",# 2 differents Maud Thomas \n",
    "    \"a5043113193\",\n",
    "    \"a5041522495\", # 3 differents importants Olivier Lopez\n",
    "    \"a5049226969\",\n",
    "    \"a5011379612\",\n",
    "    \"a5029271059\",\n",
    "    \"a5043406295\",\n",
    "    \"a5051516688\", #  Tien Tuan Dao  pas de mention SU\n",
    "    \"a5027346778\",\n",
    "    \"a5040717345\",\n",
    "    \"a5048594819\", # 2 differents AThierry Dufour \n",
    "    \"a5013928164\", # 2 differents Abdenour Hadid \n",
    "    \"a5090882219\",\n",
    "    \"a5024260775\",\n",
    "    \"a5051963748\",\n",
    "    \"a5005868901\",\n",
    "    \"a5044064042\", # 2 differents  Franz Manni  \n",
    "    \"a5027364763\", # 2 differents Grace Younes \n",
    "    \"a5033136511\",\n",
    "    \"a5076437822\",\n",
    "    \"a5064018319\",\n",
    "    \"a5049226428\",\n",
    "    \"a5000802564\",\n",
    "    \"a5050418541\", # 2 differents Philippe Bonnifait \n",
    "    \"a5085744367\",\n",
    "    \"a5054524844\", # pas de mention de SU Philippe Trigano\n",
    "    \"a5078607983\", # 3 differents  Raed Abu Zitar \n",
    "    \"a5008158634\",\n",
    "    \"a5070285963\",\n",
    "    \"a5012926469\",\n",
    "    \"a5021351429\",\n",
    "    \"a5009983613\"\n",
    "]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
