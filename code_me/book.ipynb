{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/boudinfl/pke.git\n",
      "  Cloning https://github.com/boudinfl/pke.git to c:\\users\\xavmo\\appdata\\local\\temp\\pip-req-build-t34qjshb\n",
      "  Resolved https://github.com/boudinfl/pke.git to commit 69871ffdb720b83df23684fea53ec8776fd87e63\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: nltk in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (3.8.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (3.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.4.2)\n",
      "Requirement already satisfied: unidecode in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.2.0)\n",
      "Collecting future (from pke==2.0.0)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.4.2)\n",
      "Collecting spacy>=3.2.3 (from pke==2.0.0)\n",
      "  Downloading spacy-3.7.5-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading murmurhash-1.0.10-cp312-cp312-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading cymem-2.0.8-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading thinc-8.2.5-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading srsly-2.4.8-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (23.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: click in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from nltk->pke==2.0.0) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from nltk->pke==2.0.0) (2023.10.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from scikit-learn->pke==2.0.0) (2.2.0)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2024.7.4)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading blis-0.7.11-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy>=3.2.3->pke==2.0.0) (0.4.6)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (13.3.5)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading cloudpathlib-0.18.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (2.1.3)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.2.3->pke==2.0.0)\n",
      "  Downloading marisa_trie-1.2.0-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (0.1.0)\n",
      "Downloading spacy-3.7.5-cp312-cp312-win_amd64.whl (11.7 MB)\n",
      "   ---------------------------------------- 0.0/11.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/11.7 MB 1.7 MB/s eta 0:00:08\n",
      "   ---------------------------------------- 0.1/11.7 MB 1.7 MB/s eta 0:00:08\n",
      "   ---------------------------------------- 0.1/11.7 MB 804.6 kB/s eta 0:00:15\n",
      "    --------------------------------------- 0.2/11.7 MB 1.1 MB/s eta 0:00:11\n",
      "    --------------------------------------- 0.3/11.7 MB 1.1 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.4/11.7 MB 1.4 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.7/11.7 MB 2.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.0/11.7 MB 2.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.4/11.7 MB 3.2 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.6/11.7 MB 3.3 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.9/11.7 MB 3.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.1/11.7 MB 3.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.1/11.7 MB 3.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.3/11.7 MB 3.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.4/11.7 MB 3.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.4/11.7 MB 3.3 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.5/11.7 MB 3.3 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.6/11.7 MB 3.2 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.7/11.7 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.1/11.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.3/11.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.7/11.7 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 4.0/11.7 MB 3.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.2/11.7 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.5/11.7 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.8/11.7 MB 4.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/11.7 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.6/11.7 MB 4.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.9/11.7 MB 4.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.2/11.7 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.5/11.7 MB 4.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.8/11.7 MB 4.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.2/11.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.6/11.7 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.9/11.7 MB 4.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.2/11.7 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.4/11.7 MB 4.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.7/11.7 MB 4.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.7/11.7 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.8/11.7 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.9/11.7 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.9/11.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.9/11.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.9/11.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.9/11.7 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.1/11.7 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.2/11.7 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.6/11.7 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.0/11.7 MB 4.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.3/11.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.7/11.7 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.0/11.7 MB 4.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.7 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.7 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.7/11.7 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.7/11.7 MB 4.7 MB/s eta 0:00:00\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "   ---------------------------------------- 0.0/491.3 kB ? eta -:--:--\n",
      "   --------------------------------------  481.3/491.3 kB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 491.3/491.3 kB 6.2 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/182.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 182.0/182.0 kB 3.7 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.10-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.4/122.4 kB 3.5 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp312-cp312-win_amd64.whl (478 kB)\n",
      "   ---------------------------------------- 0.0/478.8 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 378.9/478.8 kB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 478.8/478.8 kB 5.0 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.5-cp312-cp312-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.4/1.4 MB 9.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.8/1.4 MB 8.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.2/1.4 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 6.6 MB/s eta 0:00:00\n",
      "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.2/47.2 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.3/50.3 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading blis-0.7.11-cp312-cp312-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/6.6 MB 4.1 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.6/6.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.9/6.6 MB 6.1 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.2/6.6 MB 6.5 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.6/6.6 MB 6.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.9/6.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.9/6.6 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.1/6.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.3/6.6 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.4/6.6 MB 5.2 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.5/6.6 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.7/6.6 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.0/6.6 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.3/6.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.7/6.6 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.0/6.6 MB 5.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.4/6.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.8/6.6 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.9/6.6 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.5/6.6 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.2/6.6 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.5/6.6 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.3/47.3 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.0/5.4 MB 31.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.8/5.4 MB 23.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.8/5.4 MB 22.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.7/5.4 MB 21.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.1/5.4 MB 23.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 20.2 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.0-cp312-cp312-win_amd64.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/151.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 151.1/151.1 kB 8.8 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: pke\n",
      "  Building wheel for pke (setup.py): started\n",
      "  Building wheel for pke (setup.py): finished with status 'done'\n",
      "  Created wheel for pke: filename=pke-2.0.0-py3-none-any.whl size=6161127 sha256=137be721eb7471a96ad20e3f300de86689d0104e31826bc15b3298288c8a9305\n",
      "  Stored in directory: C:\\Users\\xavmo\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-q9g__hgw\\wheels\\2e\\78\\39\\76193c2a815f4cf34d67af2d338910453ecae3bde545185b65\n",
      "Successfully built pke\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, future, cloudpathlib, catalogue, blis, srsly, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy, pke\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.18.1 confection-0.1.5 cymem-2.0.8 future-1.0.0 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 murmurhash-1.0.10 pke-2.0.0 preshed-3.0.9 shellingham-1.5.4 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 typer-0.12.3 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/boudinfl/pke.git 'C:\\Users\\xavmo\\AppData\\Local\\Temp\\pip-req-build-t34qjshb'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ------------------------------------  972.8/981.5 kB 30.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- 981.5/981.5 kB 20.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993255 sha256=e93cde08b0ae19deee4d1086b3b6f8eaed7487aba1704c5d3a384dfc8385e805\n",
      "  Stored in directory: c:\\users\\xavmo\\appdata\\local\\pip\\cache\\wheels\\c1\\67\\88\\e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "Collecting polars\n",
      "  Downloading polars-1.1.0-cp38-abi3-win_amd64.whl.metadata (14 kB)\n",
      "Downloading polars-1.1.0-cp38-abi3-win_amd64.whl (30.5 MB)\n",
      "   ---------------------------------------- 0.0/30.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.9/30.5 MB 28.4 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 2.5/30.5 MB 31.6 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 3.1/30.5 MB 28.7 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 4.1/30.5 MB 24.1 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 5.5/30.5 MB 25.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 7.5/30.5 MB 28.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 9.3/30.5 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 10.5/30.5 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 12.1/30.5 MB 29.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 13.5/30.5 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 15.0/30.5 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 16.9/30.5 MB 34.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 19.0/30.5 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 20.9/30.5 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 23.0/30.5 MB 40.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 24.8/30.5 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 26.4/30.5 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 28.3/30.5 MB 40.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 29.3/30.5 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  30.5/30.5 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  30.5/30.5 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  30.5/30.5 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  30.5/30.5 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  30.5/30.5 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  30.5/30.5 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  30.5/30.5 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 30.5/30.5 MB 18.2 MB/s eta 0:00:00\n",
      "Installing collected packages: polars\n",
      "Successfully installed polars-1.1.0\n",
      "Collecting groq\n",
      "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from groq) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from groq) (2.5.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from groq) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.14.6)\n",
      "Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
      "   ---------------------------------------- 0.0/103.5 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 41.0/103.5 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 41.0/103.5 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 41.0/103.5 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 41.0/103.5 kB ? eta -:--:--\n",
      "   -------------------------------------  102.4/103.5 kB 454.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- 103.5/103.5 kB 396.7 kB/s eta 0:00:00\n",
      "Installing collected packages: groq\n",
      "Successfully installed groq-0.9.0\n",
      "Requirement already satisfied: pyarrow in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (14.0.2)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pyarrow) (1.26.4)\n",
      "Collecting fastparquet\n",
      "  Downloading fastparquet-2024.5.0.tar.gz (466 kB)\n",
      "     ---------------------------------------- 0.0/466.9 kB ? eta -:--:--\n",
      "     ---------------- --------------------- 204.8/466.9 kB 4.1 MB/s eta 0:00:01\n",
      "     ------------------------------ ------- 368.6/466.9 kB 3.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 466.9/466.9 kB 3.7 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from fastparquet) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from fastparquet) (1.26.4)\n",
      "Collecting cramjam>=2.3 (from fastparquet)\n",
      "  Downloading cramjam-2.8.3-cp312-none-win_amd64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from fastparquet) (2024.3.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from fastparquet) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Downloading cramjam-2.8.3-cp312-none-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.9/1.6 MB 27.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.0/1.6 MB 22.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 14.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 11.6 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: fastparquet\n",
      "  Building wheel for fastparquet (pyproject.toml): started\n",
      "  Building wheel for fastparquet (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for fastparquet: filename=fastparquet-2024.5.0-cp312-cp312-win_amd64.whl size=672219 sha256=3013910964bdeeff8a25a2b3d2dec5673344536198429611b2552f06604f46fb\n",
      "  Stored in directory: c:\\users\\xavmo\\appdata\\local\\pip\\cache\\wheels\\20\\a5\\e7\\6a0a900afd0174aa4ea7617b98be2ef0afd9698fed4ea0c351\n",
      "Successfully built fastparquet\n",
      "Installing collected packages: cramjam, fastparquet\n",
      "Successfully installed cramjam-2.8.3 fastparquet-2024.5.0\n",
      "Requirement already satisfied: pandas in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\xavmo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/boudinfl/pke.git\n",
    "!pip install langdetect\n",
    "!pip install polars\n",
    "!pip install groq\n",
    "!pip install pyarrow\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies nécessaires\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from langdetect import detect\n",
    "import json\n",
    "import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation d'un corpus : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorbonne Université à tous son corpus de publications principalement sur HAL.  \n",
    "Cependant, suite à une annonce de partenariat avec OpenAlex et allant de plus en plus sur cette bibliothèque gratuite et open source, le choix d'aller chercher dans un premier temps mon corpus dessus est donc le plus pertinant.  \n",
    "Dans un premier temps, il nous faut donc comparer la différence des mots clés utilisés dans HAL et OpenAlex via le travail de Nacef effectué sur le github suivant (https://github.com/obtic-sorbonne/keywords/blob/main/TER_Keyword_Extraction_sur_BDD_HAL.ipynb).  \n",
    "Regardons un premier temps quelles sont les métadonnées des publications sur HAL et OA en prenant pour exemple une publication de Stephane Le Crom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'docid': '122816', 'label_s': 'Marika Kapsimali, Stéphane Le Crom, Philippe Vernier. A natural history of vertebrate dopamine receptors.. Anita Sidhu, Marc Laruelle, Philippe Vernier. Dopamine Receptors and Transporters, Marcel Dekker Inc, pp.1-45, 2003. &#x27E8;hal-00122816&#x27E9;', 'uri_s': 'https://hal.science/hal-00122816'}\n",
      "docid\n",
      "label_s\n",
      "uri_s\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "hal_id = 'hal-00122816'\n",
    "url = f\"https://api.archives-ouvertes.fr/search/?q=halId_s:{hal_id}&wt=json\"\n",
    "\n",
    "response = requests.get(url)\n",
    "# Faire la requête à l'API\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    # Conversion de la réponse en JSON\n",
    "    data = json.loads(response.text)\n",
    "    # Prendre le premier document (s'il y en a plusieurs)\n",
    "    doc = data['response']['docs'][0]\n",
    "    print(doc)\n",
    "    # Vérifier s'il y a des documents dans la réponse\n",
    "    if data['response']['numFound'] > 0:\n",
    "        \n",
    "        # Afficher les noms de toutes les métadonnées\n",
    "        for key in doc.keys():\n",
    "            print(key)\n",
    "    else:\n",
    "        print(\"Aucune publication trouvée avec cet identifiant HAL.\")\n",
    "else:\n",
    "    print(f\"Erreur lors de la requête : {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['meta', 'results', 'group_by'])\n",
      "dict_keys(['id', 'doi', 'title', 'display_name', 'publication_year', 'publication_date', 'ids', 'language', 'primary_location', 'type', 'type_crossref', 'indexed_in', 'open_access', 'authorships', 'countries_distinct_count', 'institutions_distinct_count', 'corresponding_author_ids', 'corresponding_institution_ids', 'apc_list', 'apc_paid', 'fwci', 'has_fulltext', 'cited_by_count', 'cited_by_percentile_year', 'biblio', 'is_retracted', 'is_paratext', 'primary_topic', 'topics', 'keywords', 'concepts', 'mesh', 'locations_count', 'locations', 'best_oa_location', 'sustainable_development_goals', 'grants', 'datasets', 'versions', 'referenced_works_count', 'referenced_works', 'related_works', 'ngrams_url', 'abstract_inverted_index', 'cited_by_api_url', 'counts_by_year', 'updated_date', 'created_date'])\n"
     ]
    }
   ],
   "source": [
    "# ID de l'auteur\n",
    "author_id = \"a5007814380\"  \n",
    "\n",
    "# URL de l'API\n",
    "url = f\"https://api.openalex.org/works?filter=author.id:{author_id}\"\n",
    "\n",
    "# Faire la requête à l'API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Vérifier que la requête a réussi\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    # Conversion de la réponse en JSON\n",
    "    data = json.loads(response.text)\n",
    "    print(data.keys())\n",
    "    i=1\n",
    "    for work in data['results'] : \n",
    "        if i==1 :\n",
    "            print(work.keys())\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  On voit qu'il nous ait bien compliqué d'obtenir par code tous les paramètres des métadonnées d'une publication sur HAL. Je vous encourage donc à suivre\n",
    "le lien suivant afin de mieux comprendre les différents paramètres renseignés initialement (https://hal.science/help/information).  \n",
    "Nous remarquons à présent avec la seconde brique de code tous les différents paramètres renseignés par les métadonnées d'une publication d'OpenAlex. Celles-ci sont nettement plus complexes et intéressantes à utiliser que celles de HAL, et ce pour de multiples raisons :  \n",
    "- <ins>**Keywords et scorings**</ins> :  Keywords choisis par Bert, en fonction d'un scoring indiqué. </br> \n",
    "</br> \n",
    "- <ins>**Topics**</ins> : Le topics d'OpenAlex ont été créés manuellement et les publications y sont classées par clustering ( pour plus de renseignement je vous encourage à lire le document suivant mise en ligne par OA: https://docs.google.com/document/d/1bDopkhuGieQ4F8gGNj7sEc8WSE8mvLZS/edit#heading=h.5w2tb5fcg77r) </br>\n",
    "</br>\n",
    "-  <ins>**Citations** </ins> : OA est la seule librairie en ligne où il y a la présence des références dans les métadonnées, c'est-à-dire à la fois les travaux qui le référence ou à contrario qu'il référence. Cela est d'une grande aide pour faire la liaison entre ces différents travaux. Cet ajout permet de faciliter la récupération des citation et donc d'appliquer l'algorithme de Leiden plus aisément.</br> \n",
    "\n",
    "Dans un premier temps, une étude des keywords en fonction d'HAL et d'OpenAlex sera faite afin de verifier la pertinence de leurs attributions. Pour ce\n",
    "faire, nous appliquerons un raisonnement par induction. Nous sélectionnerons un plus petit corpus constitué des travaux de Stéphane Le Crom afin d'obtenir des premiers résultats rapides. Puis, nous appliquerons notre protocole sur notre corpus global pour une comparaison à plus grande échelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quel protocole utiliser ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a deux lvl pour keywords -> mots clés exacts similaires ou etymologiquement similaire (via du LLM ou scoring ??)\n",
    "#                               -> comparaison par topic de ses mots clés d'open alex (Comparaison avec les autres topics??)\n",
    "#                               -> comment calculer la similarité? calculer par mbert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des publications de Stéphane Le Crom sur HAL et OpenAlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de publications récupérées : 64\n"
     ]
    }
   ],
   "source": [
    "auteurs = [\"Stéphane Le Crom\"]\n",
    "\n",
    "# Initialisez une liste vide pour stocker les métadonnées des publications\n",
    "publications = []\n",
    "\n",
    "# URL de base de l'API HAL\n",
    "base_url = \"https://api.archives-ouvertes.fr/search/\"\n",
    "\n",
    "# Parcourez la liste des auteurs et récupérez leurs publications\n",
    "for auteur in auteurs:\n",
    "    params = {\n",
    "        \"q\": f'authFullName_s:\"{auteur}\"',\n",
    "        \"fl\": \"authFullName_s,authIdHal_i,authIdHal_s,title_s,halId_s,producedDateY_i,doiId_s,abstract_s,uri_s,domain_s,keyword_s\",\n",
    "        \"rows\": 10000  # Augmentez le nombre de lignes si nécessaire\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        publications.extend(data[\"response\"][\"docs\"])\n",
    "    else:\n",
    "        print(f\"Erreur pour l'auteur {auteur}: {response.status_code} - {response.text}\")\n",
    "\n",
    "# Créez un DataFrame Pandas à partir des métadonnées des publications\n",
    "column_order = [\"authFullName_s\", \"authIdHal_i\", \"authIdHal_s\", \"title_s\", \"halId_s\", \"producedDateY_i\", \"doiId_s\", \"keyword_s\", \"abstract_s\", \"uri_s\", \"domain_s\"]\n",
    "\n",
    "df = pd.DataFrame(publications, columns= column_order)\n",
    "\n",
    "# Sauvegardez le DataFrame dans un fichier CSV\n",
    "df.to_csv(\"publications_hal_stephane_le_crom.csv\", index=False)\n",
    "\n",
    "# Affichez le nombre total de publications récupérées\n",
    "print(f\"Nombre total de publications récupérées : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publications restantes après nettoyage : 62\n"
     ]
    }
   ],
   "source": [
    "# Conversion les listes en chaînes dans la colonne \"title_s\"\n",
    "df[\"title_s\"] = df[\"title_s\"].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Suppression des doublons à base du titre seulement\n",
    "df = df.drop_duplicates(subset=\"title_s\")\n",
    "print(f\"Publications restantes après nettoyage : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID de l'auteur\n",
    "author_id = \"a5052835091\"\n",
    "\n",
    "# URL de l'API\n",
    "url = f\"https://api.openalex.org/works?filter=author.id:{author_id}&per-page=200\"\n",
    "\n",
    "# Faire la requête à l'API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Vérifier que la requête a réussi\n",
    "if response.status_code == 200:\n",
    "    # Conversion de la réponse en JSON\n",
    "    data = json.loads(response.text)\n",
    "\n",
    "    # Sélection les métadonnées précises\n",
    "    selected_data = []\n",
    "    \n",
    "    for work in data['results']:\n",
    "        \n",
    "        author_ids = []\n",
    "        if 'authorships' in work: \n",
    "            for authorship in work['authorships']:\n",
    "                # Ajoutez l'identifiant de l'auteur à la liste 'author_ids'\n",
    "                author_ids.append(authorship[\"author\"][\"id\"])\n",
    "                \n",
    "        keywords, scorings = [], [] \n",
    "        if 'keywords' in work:\n",
    "            for id in work['keywords']:\n",
    "                keywords.append(id['display_name'])\n",
    "                scorings.append(id['score'])\n",
    "                \n",
    "        selected_data.append({\n",
    "            'id' : work['id'],\n",
    "            'doi': work['doi'],\n",
    "            'title': work['title'],\n",
    "            'authors_id' : author_ids, \n",
    "            'keywords' : keywords, \n",
    "            'scorings' : scorings,\n",
    "            'referenced_works': work['referenced_works'], \n",
    "            'related_works': work['related_works'],\n",
    "            'cited_by_api_url': work['cited_by_api_url'],    \n",
    "        })\n",
    "\n",
    "    # Conversion des données sélectionnées en DataFrame\n",
    "    df2 = pd.DataFrame(selected_data)\n",
    "\n",
    "    # Exportation du DataFrame en CSV\n",
    "    df2.to_csv('publications_oa_stephane_le_crom.csv', index=False)\n",
    "else:\n",
    "    print(f\"Erreur : {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les publications ont été prélevés sur HAL et OA, il faut donc les lier entre elles afin de pouvoir les comparer efficacement. A ça, deux solutions possibles :  \n",
    "- <ins>**Par le titre**</ins> : le plus évident mais que des points négatifs. Le code pour nettoyer un titre est conséquent (comme on peut le voir dans le bloc suivant, inspiré de celui d'openalex). Parmi tous ces points négatifs, on peut y trouver la différence de langue pour une même publication, l'apparition de caractères non latin, ...  \n",
    "<br>\n",
    "- <ins>**Par le DOI**</ins> : Le DOI est unique pour chaque publication quelque soit l'endroit où il est posté. Cela présente donc la meilleure solution pour lier les documents. Pour effectuer cette tache au mieux, il faut nettoyer le DOI des publications d'OA. Celui-ci est un lien composer d'un début de requête API suivi du DOI. Nous devons donc faire en sorte de ne conserver que le DOI. De plus, nous supprimerons dans le dataset de HAL toutes les publications sans DOI ainsi que les dupliquer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_keep_ind(groups):\n",
    "    \"\"\"\n",
    "    Function to determine if a text should be kept or not.\n",
    "\n",
    "    Input:\n",
    "    groups: list of character groups\n",
    "\n",
    "    Output:\n",
    "    0: if text should be not used\n",
    "    1: if text should be used\n",
    "    \"\"\"\n",
    "    # Groups of characters that do not perform well\n",
    "    groups_to_skip = ['HIRAGANA', 'CJK', 'KATAKANA','ARABIC', 'HANGUL', 'THAI','DEVANAGARI','BENGALI',\n",
    "                      'THAANA','GUJARATI','CYRILLIC']\n",
    "    \n",
    "    if any(x in groups_to_skip for x in groups):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def remove_non_latin_characters(text):\n",
    "    \"\"\"\n",
    "    Function to remove non-latin characters.\n",
    "\n",
    "    Input:\n",
    "    text: string of characters\n",
    "\n",
    "    Output:\n",
    "    final_char: string of characters with non-latin characters removed\n",
    "    \"\"\"\n",
    "    final_char = []\n",
    "    groups_to_skip = ['HIRAGANA', 'CJK', 'KATAKANA','ARABIC', 'HANGUL', 'THAI','DEVANAGARI','BENGALI',\n",
    "                      'THAANA','GUJARATI','CYRILLIC']\n",
    "    for char in text:\n",
    "        try:\n",
    "            script = unicodedata.name(char).split(\" \")[0]\n",
    "            if script not in groups_to_skip:\n",
    "                final_char.append(char)\n",
    "        except:\n",
    "            pass\n",
    "    return \"\".join(final_char)\n",
    "    \n",
    "def group_non_latin_characters(text):\n",
    "    \"\"\"\n",
    "    Function to group non-latin characters and return the number of latin characters.\n",
    "\n",
    "    Input:\n",
    "    text: string of characters\n",
    "\n",
    "    Output:\n",
    "    groups: list of character groups\n",
    "    latin_chars: number of latin characters\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    latin_chars = []\n",
    "    text = text.replace(\".\", \"\").replace(\" \", \"\")\n",
    "    for char in text:\n",
    "        try:\n",
    "            script = unicodedata.name(char).split(\" \")[0]\n",
    "            if script == 'LATIN':\n",
    "                latin_chars.append(script)\n",
    "            else:\n",
    "                if script not in groups:\n",
    "                    groups.append(script)\n",
    "        except:\n",
    "            if \"UNK\" not in groups:\n",
    "                groups.append(\"UNK\")\n",
    "    return groups, len(latin_chars)\n",
    "\n",
    "def check_for_non_latin_characters(text):\n",
    "    \"\"\"\n",
    "    Function to check if non-latin characters are dominant in a text.\n",
    "\n",
    "    Input:\n",
    "    text: string of characters\n",
    "\n",
    "    Output:\n",
    "    0: if text should be not used\n",
    "    1: if text should be used\n",
    "    \"\"\"\n",
    "    groups, latin_chars = group_non_latin_characters(str(text))\n",
    "    if name_to_keep_ind(groups) == 1:\n",
    "        return 1\n",
    "    elif latin_chars > 20:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def clean_title(old_title):\n",
    "    \"\"\"\n",
    "    Function to check if title should be kept and then remove non-latin characters. Also\n",
    "    removes some HTML tags from the title.\n",
    "    \n",
    "    Input:\n",
    "    old_title: string of title\n",
    "    \n",
    "    Output:\n",
    "    new_title: string of title with non-latin characters and HTML tags removed\n",
    "    \"\"\"\n",
    "    keep_title = check_for_non_latin_characters(old_title)\n",
    "    if (keep_title == 1) & isinstance(old_title, str):\n",
    "        new_title = remove_non_latin_characters(old_title)\n",
    "        if '<' in new_title:\n",
    "            new_title = new_title.replace(\"<i>\", \"\").replace(\"</i>\",\"\")\\\n",
    "                                 .replace(\"<sub>\", \"\").replace(\"</sub>\",\"\") \\\n",
    "                                 .replace(\"<sup>\", \"\").replace(\"</sup>\",\"\") \\\n",
    "                                 .replace(\"<em>\", \"\").replace(\"</em>\",\"\") \\\n",
    "                                 .replace(\"<b>\", \"\").replace(\"</b>\",\"\") \\\n",
    "                                 .replace(\"<I>\", \"\").replace(\"</I>\", \"\") \\\n",
    "                                 .replace(\"<SUB>\", \"\").replace(\"</SUB>\", \"\") \\\n",
    "                                 .replace(\"<scp>\", \"\").replace(\"</scp>\", \"\") \\\n",
    "                                 .replace(\"<font>\", \"\").replace(\"</font>\", \"\") \\\n",
    "                                 .replace(\"<inf>\",\"\").replace(\"</inf>\", \"\") \\\n",
    "                                 .replace(\"<i /> \", \"\") \\\n",
    "                                 .replace(\"<p>\", \"\").replace(\"</p>\",\"\") \\\n",
    "                                 .replace(\"<![CDATA[<B>\", \"\").replace(\"</B>]]>\", \"\") \\\n",
    "                                 .replace(\"<italic>\", \"\").replace(\"</italic>\",\"\")\\\n",
    "                                 .replace(\"<title>\", \"\").replace(\"</title>\", \"\") \\\n",
    "                                 .replace(\"<br>\", \"\").replace(\"</br>\",\"\").replace(\"<br/>\",\"\") \\\n",
    "                                 .replace(\"<B>\", \"\").replace(\"</B>\", \"\") \\\n",
    "                                 .replace(\"<em>\", \"\").replace(\"</em>\", \"\") \\\n",
    "                                 .replace(\"<BR>\", \"\").replace(\"</BR>\", \"\") \\\n",
    "                                 .replace(\"<title>\", \"\").replace(\"</title>\", \"\") \\\n",
    "                                 .replace(\"<strong>\", \"\").replace(\"</strong>\", \"\") \\\n",
    "                                 .replace(\"<formula>\", \"\").replace(\"</formula>\", \"\") \\\n",
    "                                 .replace(\"<roman>\", \"\").replace(\"</roman>\", \"\") \\\n",
    "                                 .replace(\"<SUP>\", \"\").replace(\"</SUP>\", \"\") \\\n",
    "                                 .replace(\"<SSUP>\", \"\").replace(\"</SSUP>\", \"\") \\\n",
    "                                 .replace(\"<sc>\", \"\").replace(\"</sc>\", \"\") \\\n",
    "                                 .replace(\"<subtitle>\", \"\").replace(\"</subtitle>\", \"\") \\\n",
    "                                 .replace(\"<emph/>\", \"\").replace(\"<emph>\", \"\").replace(\"</emph>\", \"\") \\\n",
    "                                 .replace(\"\"\"<p class=\"Body\">\"\"\", \"\") \\\n",
    "                                 .replace(\"<TITLE>\", \"\").replace(\"</TITLE>\", \"\") \\\n",
    "                                 .replace(\"<sub />\", \"\").replace(\"<sub/>\", \"\") \\\n",
    "                                 .replace(\"<mi>\", \"\").replace(\"</mi>\", \"\") \\\n",
    "                                 .replace(\"<bold>\", \"\").replace(\"</bold>\", \"\") \\\n",
    "                                 .replace(\"<mtext>\", \"\").replace(\"</mtext>\", \"\") \\\n",
    "                                 .replace(\"<msub>\", \"\").replace(\"</msub>\", \"\") \\\n",
    "                                 .replace(\"<mrow>\", \"\").replace(\"</mrow>\", \"\") \\\n",
    "                                 .replace(\"</mfenced>\", \"\").replace(\"</math>\", \"\")\n",
    "\n",
    "            if '<mml' in new_title:\n",
    "                all_parts = [x for y in [i.split(\"mml:math>\") for i in new_title.split(\"<mml:math\")] for x in y if x]\n",
    "                final_parts = []\n",
    "                for part in all_parts:\n",
    "                    if re.search(r\"\\>[$%#!^*\\w.,/()+-]*\\<\", part):\n",
    "                        pull_out = re.findall(r\"\\>[$%#!^*\\w.,/()+-]*\\<\", part)\n",
    "                        final_pieces = []\n",
    "                        for piece in pull_out:\n",
    "                            final_pieces.append(piece.replace(\">\", \"\").replace(\"<\", \"\"))\n",
    "                        \n",
    "                        final_parts.append(\" \"+ \"\".join(final_pieces) + \" \")\n",
    "                    else:\n",
    "                        final_parts.append(part)\n",
    "                \n",
    "                new_title = \"\".join(final_parts).strip()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if '<xref' in new_title:\n",
    "                new_title = re.sub(r\"\\<xref[^/]*\\/xref\\>\", \"\", new_title)\n",
    "\n",
    "            if '<inline-formula' in new_title:\n",
    "                new_title = re.sub(r\"\\<inline-formula[^/]*\\/inline-formula\\>\", \"\", new_title)\n",
    "\n",
    "            if '<title' in new_title:\n",
    "                new_title = re.sub(r\"\\<title[^/]*\\/title\\>\", \"\", new_title)\n",
    "\n",
    "            if '<p class=' in new_title:\n",
    "                new_title = re.sub(r\"\\<p class=[^>]*\\>\", \"\", new_title)\n",
    "            \n",
    "            if '<span class=' in new_title:\n",
    "                new_title = re.sub(r\"\\<span class=[^>]*\\>\", \"\", new_title)\n",
    "\n",
    "            if 'mfenced open' in new_title:\n",
    "                new_title = re.sub(r\"\\<mfenced open=[^>]*\\>\", \"\", new_title)\n",
    "            \n",
    "            if 'math xmlns' in new_title:\n",
    "                new_title = re.sub(r\"\\<math xmlns=[^>]*\\>\", \"\", new_title)\n",
    "\n",
    "        if '<' in new_title:\n",
    "            new_title = new_title.replace(\">i<\", \"\").replace(\">/i<\", \"\") \\\n",
    "                                 .replace(\">b<\", \"\").replace(\">/b<\", \"\") \\\n",
    "                                 .replace(\"<inline-formula>\", \"\").replace(\"</inline-formula>\",\"\")\n",
    "        if new_title.isupper():\n",
    "            new_title = new_title.title()\n",
    "        \n",
    "        return new_title\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def liaison_second(df1_hal, df2_oa):\n",
    "    # On clean les titres de la df d'OpenAlex\n",
    "    for i, title in enumerate(df2_oa['title']):\n",
    "        new_title = clean_title(title)  # clean_title : la fonction de nettoyage appropriée\n",
    "        df2_oa.loc[i, 'title'] = new_title\n",
    "\n",
    "    # On convertit les titres d'oa en liste\n",
    "    list_oa_title = df2_oa['title'].tolist()\n",
    "\n",
    "    # On supprime toutes les publications qui ne sont pas en commun\n",
    "    indexes_to_drop = []\n",
    "    for i, title in enumerate(df1_hal['title_s']):\n",
    "        if title not in list_oa_title:\n",
    "            indexes_to_drop.append(i)\n",
    "\n",
    "    df1_hal.drop(indexes_to_drop, inplace=True)\n",
    "    \n",
    "def clean_doi_oa(old_doi):\n",
    "    if old_doi is not None:\n",
    "        return old_doi.replace(\"https://doi.org/\", \"\")\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def clean_doi_oa2(df):\n",
    "    df.dropna(subset=['doi'], inplace=True)\n",
    "    df.drop_duplicates(subset=['doi'], inplace=True)\n",
    "    \n",
    "def clean_doi_hal(df):\n",
    "    df.dropna(subset=['doiId_s'], inplace=True)\n",
    "    df.drop_duplicates(subset=['doiId_s'], inplace=True)\n",
    "\n",
    "def liaison(df1_hal, df2_oa):\n",
    "    \n",
    "    # On clean les DOI de la df d'OpenAlex\n",
    "    for i, doi in enumerate(df2_oa['doi']):\n",
    "        new_doi = clean_doi_oa(doi)\n",
    "        df2_oa.loc[i, 'doi'] = new_doi\n",
    "\n",
    "    \n",
    "    # On convertit les DOI d'HAL en liste\n",
    "    list_hal_doi = df1_hal['doiId_s'].tolist()\n",
    "    # Création de la nouvelle df de stockage\n",
    "    df2_change = pd.DataFrame()\n",
    "\n",
    "    # On supprime toutes les publications qui ne sont pas en commun\n",
    "    indexes_to_drop = []\n",
    "    \n",
    "    for i, doi in enumerate(df2_oa['doi']):\n",
    "        if doi in list_hal_doi:\n",
    "            matching_rows = df1_hal[df1_hal['doiId_s'] == doi]\n",
    "            if not matching_rows.empty:\n",
    "                index_in_df1_hal = matching_rows.index[0]\n",
    "                df2_change = pd.concat([df2_change, df1_hal.iloc[[index_in_df1_hal]]])\n",
    "        else : \n",
    "            indexes_to_drop.append(i)\n",
    "     \n",
    "    print(\"ca passe\")       \n",
    "    df2_oa.drop(indexes_to_drop, inplace=True)\n",
    "    df2_change.to_csv('pub_hal_liees.csv', index=False)\n",
    "    \n",
    "    return df2_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "----- Clean df oa -----\n",
      "-----------------------\n",
      "(102, 9)\n",
      "(102, 9)\n",
      "-----------------------\n",
      "---- Clean df HAL -----\n",
      "-----------------------\n",
      "(47, 11)\n",
      "(47, 11)\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------\")\n",
    "print(\"----- Clean df oa -----\")\n",
    "print(\"-----------------------\")\n",
    "print(df2.shape)\n",
    "clean_doi_oa2(df2)\n",
    "print(df2.shape)\n",
    "print(\"-----------------------\")\n",
    "print(\"---- Clean df HAL -----\")\n",
    "print(\"-----------------------\")\n",
    "print(df.shape)\n",
    "clean_doi_hal(df)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification de l'unicité de chaque DOI de HAL\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Verification de l'unicité de chaque DOI de HAL\")\n",
    "is_all_unique = df['doiId_s'].nunique() == len(df['doiId_s'])\n",
    "print(is_all_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\xavmo\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1714\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1713\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1716\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\xavmo\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4153\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   4144\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4145\u001b[0m \u001b[38;5;124;03mInternal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   4146\u001b[0m \u001b[38;5;124;03mattribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4151\u001b[0m \u001b[38;5;124;03mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   4152\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4153\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indices\u001b[38;5;241m=\u001b[39mindices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   4154\u001b[0m \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\xavmo\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4131\u001b[0m     )\n\u001b[1;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mtake(\n\u001b[0;32m   4134\u001b[0m     indices,\n\u001b[0;32m   4135\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_block_manager_axis(axis),\n\u001b[0;32m   4136\u001b[0m     verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   4137\u001b[0m )\n\u001b[0;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4140\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\xavmo\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:891\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    890\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[axis]\n\u001b[1;32m--> 891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n",
      "File \u001b[1;32mc:\\Users\\xavmo\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexers\\utils.py:282\u001b[0m, in \u001b[0;36mmaybe_convert_indices\u001b[1;34m(indices, n, verify)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n",
      "\u001b[1;31mIndexError\u001b[0m: indices are out-of-bounds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df2_change \u001b[38;5;241m=\u001b[39m liaison(df,df2)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df2_change))\n",
      "Cell \u001b[1;32mIn[92], line 244\u001b[0m, in \u001b[0;36mliaison\u001b[1;34m(df1_hal, df2_oa)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matching_rows\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m    243\u001b[0m         index_in_df1_hal \u001b[38;5;241m=\u001b[39m matching_rows\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 244\u001b[0m         df2_change \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df2_change, df1_hal\u001b[38;5;241m.\u001b[39miloc[[index_in_df1_hal]]])\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m : \n\u001b[0;32m    246\u001b[0m     indexes_to_drop\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "File \u001b[1;32mc:\\Users\\xavmo\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\xavmo\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1743\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1741\u001b[0m \u001b[38;5;66;03m# a list of integers\u001b[39;00m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_list_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# a single integer\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1747\u001b[0m     key \u001b[38;5;241m=\u001b[39m item_from_zerodim(key)\n",
      "File \u001b[1;32mc:\\Users\\xavmo\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1717\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1716\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[39;00m\n\u001b[1;32m-> 1717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional indexers are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "df2_change = liaison(df,df2)\n",
    "print(len(df2_change))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "def extract_keywords_groq(text, models):\n",
    "    prompt = f\"As a keyword extraction master, your only mission here is to extract only the most relevent keywords that are present in the text. Put the list of keywords between brackets, comma-seperated.DO NOT write something else than the keywords you're supposed to extract from the text. Skip the preamble and provide only the best keywords (present in the text). The text:\\n{text} VERY IMPORTANT : Put the list of keywords between brackets, comma-seperated like this ['keyword1', 'keyword2, 'keyword3' etc...] NO TICKS only what ive said\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"[\"\n",
    "            }\n",
    "        ],\n",
    "        model=models,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    keyword_list = [keyword.strip(' \"') for keyword in response.strip(\"[]\").split(', ')]\n",
    "\n",
    "\n",
    "    return keyword_list\n",
    "\n",
    "def extract_groq(df, list_models, use_title = False):\n",
    "    \n",
    "    for model in list_models :\n",
    "\n",
    "        list_kw = []\n",
    "\n",
    "        for row in trange(len(df)):\n",
    "            text = df.loc[row,'abstract_s']\n",
    "            if pd.isnull(text):\n",
    "                text = ''\n",
    "            else:\n",
    "                text = str(text)\n",
    "                \n",
    "            if use_title:\n",
    "                title = df.loc[row, 'title_s']\n",
    "                if not pd.isnull(title):\n",
    "                    text += str(title)\n",
    "                    \n",
    "            kw = extract_keywords_groq(text, model)\n",
    "            list_kw.append(kw)\n",
    "            \n",
    "        list_of_dicts = [{'list of kw': keywords} for keywords in list_kw]\n",
    "        df_kw = pl.DataFrame(list_of_dicts)\n",
    "\n",
    "        df[model] = df_kw['list of kw']\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  2.33it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.31it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.04it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.88it/s]\n"
     ]
    }
   ],
   "source": [
    "models = ['llama3-8b-8192',\n",
    "          #'gemma-7b-it',\n",
    "          \"llama3-70b-8192\"]\n",
    "\n",
    "df = pd.read_csv('pub_hal_liees.csv')\n",
    "\n",
    "df_llm = extract_groq(df, models)\n",
    "df_llm_title = extract_groq(df_llm, models, use_title = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llm_title.to_csv('pub_hal_liees_keywords.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authFullName_s</th>\n",
       "      <th>authIdHal_i</th>\n",
       "      <th>authIdHal_s</th>\n",
       "      <th>title_s</th>\n",
       "      <th>halId_s</th>\n",
       "      <th>producedDateY_i</th>\n",
       "      <th>doiId_s</th>\n",
       "      <th>keyword_s</th>\n",
       "      <th>abstract_s</th>\n",
       "      <th>uri_s</th>\n",
       "      <th>domain_s</th>\n",
       "      <th>llama3-8b-8192</th>\n",
       "      <th>llama3-70b-8192</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Nathalie Lehmann', 'Sandrine Perrin', 'Clair...</td>\n",
       "      <td>[1148850, 174545, 13760, 748529, 185045]</td>\n",
       "      <td>['charlotte-berthelier', 'celine-hernandez', '...</td>\n",
       "      <td>Eoulsan 2: an efficient workflow manager for r...</td>\n",
       "      <td>hal-03784195</td>\n",
       "      <td>2021</td>\n",
       "      <td>10.1101/2021.10.13.464219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['A bstract Motivation Core sequencing facilit...</td>\n",
       "      <td>https://hal.science/hal-03784195</td>\n",
       "      <td>['0.sdv', '1.sdv.bibs']</td>\n",
       "      <td>[Eoulsan, workflow, sequencing, RNA-seq, trans...</td>\n",
       "      <td>[Eoulsan, workflow engine, sequencing data, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Jawad Merhej', 'Amandine Frigo', 'Stéphane L...</td>\n",
       "      <td>[13760, 742644, 918862, 879919]</td>\n",
       "      <td>['stephane-le-crom', 'jean-michel-camadro', 'f...</td>\n",
       "      <td>bPeaks: a bioinformatics tool to detect transc...</td>\n",
       "      <td>hal-01132624</td>\n",
       "      <td>2014</td>\n",
       "      <td>10.1002/yea.3031</td>\n",
       "      <td>['Yeasts', 'Transcription factors', 'Regulator...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://hal.science/hal-01132624</td>\n",
       "      <td>['0.sdv', '1.sdv.bbm', '0.sdv', '1.sdv.mp', '0...</td>\n",
       "      <td>[bioinformatics, transcription factor, binding...</td>\n",
       "      <td>[Chipseq, bioinformatics, transcription factor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Aurélien Bataille', 'Pierre Galichon', 'Nadj...</td>\n",
       "      <td>[919075, 183910, 1327675, 773912, 835505, 1376...</td>\n",
       "      <td>['badreddine-mohand-oumoussa', 'noemie-simon-t...</td>\n",
       "      <td>Increased Fatty Acid Oxidation in Differentiat...</td>\n",
       "      <td>hal-02565135</td>\n",
       "      <td>2018</td>\n",
       "      <td>10.1159/000490819</td>\n",
       "      <td>['Chronic Kidney Disease', 'Fatty Acid Oxidati...</td>\n",
       "      <td>['Background/aims: Fatty acid oxidation (FAO),...</td>\n",
       "      <td>https://hal.sorbonne-universite.fr/hal-02565135</td>\n",
       "      <td>['0.sdv', '0.sdv', '1.sdv.mhep']</td>\n",
       "      <td>[Fatty Acid Oxidation, kidney disease, chronic...</td>\n",
       "      <td>[Fatty Acid Oxidation, Chronic Kidney Disease,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Stéphane Le Crom', 'Wendy Schackwitz', 'Len ...</td>\n",
       "      <td>[13760, 1345702, 937937, 919335, 919336, 919327]</td>\n",
       "      <td>['stephane-le-crom']</td>\n",
       "      <td>Tracking the roots of cellulase hyperproductio...</td>\n",
       "      <td>hal-02879345</td>\n",
       "      <td>2009</td>\n",
       "      <td>10.1073/pnas.0905848106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Trichoderma reesei (teleomorph Hypocrea jeco...</td>\n",
       "      <td>https://hal.sorbonne-universite.fr/hal-02879345</td>\n",
       "      <td>['0.sdv', '1.sdv.bbm', '2.sdv.bbm.gtp', '0.sdv...</td>\n",
       "      <td>[Trichoderma, reesei, cellulases, hemicellulas...</td>\n",
       "      <td>[Trichoderma reesei, cellulases, hemicellulase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['Y. Saint-Georges', 'M. Garcia', 'T. Delaveau...</td>\n",
       "      <td>[13760, 908599]</td>\n",
       "      <td>['stephane-le-crom', 'lemoine-sophie']</td>\n",
       "      <td>Yeast mitochondrial biogenesis : a role for th...</td>\n",
       "      <td>hal-00333137</td>\n",
       "      <td>2008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://hal.science/hal-00333137</td>\n",
       "      <td>['0.sdv', '1.sdv.bbm']</td>\n",
       "      <td>[Yeast, mitochondrial, biogenesis, PUF, RNA, p...</td>\n",
       "      <td>[Yeast, mitochondrial, biogenesis, PUF, RNA, b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      authFullName_s  \\\n",
       "0  ['Nathalie Lehmann', 'Sandrine Perrin', 'Clair...   \n",
       "1  ['Jawad Merhej', 'Amandine Frigo', 'Stéphane L...   \n",
       "2  ['Aurélien Bataille', 'Pierre Galichon', 'Nadj...   \n",
       "3  ['Stéphane Le Crom', 'Wendy Schackwitz', 'Len ...   \n",
       "4  ['Y. Saint-Georges', 'M. Garcia', 'T. Delaveau...   \n",
       "\n",
       "                                         authIdHal_i  \\\n",
       "0           [1148850, 174545, 13760, 748529, 185045]   \n",
       "1                    [13760, 742644, 918862, 879919]   \n",
       "2  [919075, 183910, 1327675, 773912, 835505, 1376...   \n",
       "3   [13760, 1345702, 937937, 919335, 919336, 919327]   \n",
       "4                                    [13760, 908599]   \n",
       "\n",
       "                                         authIdHal_s  \\\n",
       "0  ['charlotte-berthelier', 'celine-hernandez', '...   \n",
       "1  ['stephane-le-crom', 'jean-michel-camadro', 'f...   \n",
       "2  ['badreddine-mohand-oumoussa', 'noemie-simon-t...   \n",
       "3                               ['stephane-le-crom']   \n",
       "4             ['stephane-le-crom', 'lemoine-sophie']   \n",
       "\n",
       "                                             title_s       halId_s  \\\n",
       "0  Eoulsan 2: an efficient workflow manager for r...  hal-03784195   \n",
       "1  bPeaks: a bioinformatics tool to detect transc...  hal-01132624   \n",
       "2  Increased Fatty Acid Oxidation in Differentiat...  hal-02565135   \n",
       "3  Tracking the roots of cellulase hyperproductio...  hal-02879345   \n",
       "4  Yeast mitochondrial biogenesis : a role for th...  hal-00333137   \n",
       "\n",
       "   producedDateY_i                    doiId_s  \\\n",
       "0             2021  10.1101/2021.10.13.464219   \n",
       "1             2014           10.1002/yea.3031   \n",
       "2             2018          10.1159/000490819   \n",
       "3             2009    10.1073/pnas.0905848106   \n",
       "4             2008                        NaN   \n",
       "\n",
       "                                           keyword_s  \\\n",
       "0                                                NaN   \n",
       "1  ['Yeasts', 'Transcription factors', 'Regulator...   \n",
       "2  ['Chronic Kidney Disease', 'Fatty Acid Oxidati...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                          abstract_s  \\\n",
       "0  ['A bstract Motivation Core sequencing facilit...   \n",
       "1                                                NaN   \n",
       "2  ['Background/aims: Fatty acid oxidation (FAO),...   \n",
       "3  ['Trichoderma reesei (teleomorph Hypocrea jeco...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             uri_s  \\\n",
       "0                 https://hal.science/hal-03784195   \n",
       "1                 https://hal.science/hal-01132624   \n",
       "2  https://hal.sorbonne-universite.fr/hal-02565135   \n",
       "3  https://hal.sorbonne-universite.fr/hal-02879345   \n",
       "4                 https://hal.science/hal-00333137   \n",
       "\n",
       "                                            domain_s  \\\n",
       "0                            ['0.sdv', '1.sdv.bibs']   \n",
       "1  ['0.sdv', '1.sdv.bbm', '0.sdv', '1.sdv.mp', '0...   \n",
       "2                   ['0.sdv', '0.sdv', '1.sdv.mhep']   \n",
       "3  ['0.sdv', '1.sdv.bbm', '2.sdv.bbm.gtp', '0.sdv...   \n",
       "4                             ['0.sdv', '1.sdv.bbm']   \n",
       "\n",
       "                                      llama3-8b-8192  \\\n",
       "0  [Eoulsan, workflow, sequencing, RNA-seq, trans...   \n",
       "1  [bioinformatics, transcription factor, binding...   \n",
       "2  [Fatty Acid Oxidation, kidney disease, chronic...   \n",
       "3  [Trichoderma, reesei, cellulases, hemicellulas...   \n",
       "4  [Yeast, mitochondrial, biogenesis, PUF, RNA, p...   \n",
       "\n",
       "                                     llama3-70b-8192  \n",
       "0  [Eoulsan, workflow engine, sequencing data, co...  \n",
       "1  [Chipseq, bioinformatics, transcription factor...  \n",
       "2  [Fatty Acid Oxidation, Chronic Kidney Disease,...  \n",
       "3  [Trichoderma reesei, cellulases, hemicellulase...  \n",
       "4  [Yeast, mitochondrial, biogenesis, PUF, RNA, b...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_llm_title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparaison semantique des mots par Space et autres ...  \n",
    "deux boucles for, on conserve la meilleur liaison entre les mots  \n",
    "sous forme de dico : [motclé1 : (motclé2, score), (motclé3, score), ...] et on conserve les scores au dessus de 0.5 \n",
    "faire un schéma? oui mais comment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similarité entre 'groupe de mots 1' et 'groupe de mots 2' est 0.9999999777900487\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Chargez le modèle et le tokenizer BERT pré-entraînés\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "# Encodez les groupes de mots en tant que vecteurs BERT\n",
    "input_ids1 = tokenizer.encode('haine', add_special_tokens=True)\n",
    "input_ids2 = tokenizer.encode('haine', add_special_tokens=True)\n",
    "\n",
    "# Transformez les listes en tenseurs PyTorch et ajoutez une dimension supplémentaire\n",
    "input_ids1 = torch.tensor([input_ids1])\n",
    "input_ids2 = torch.tensor([input_ids2])\n",
    "\n",
    "# Obtenez les sorties du modèle\n",
    "outputs1 = model(input_ids1)\n",
    "outputs2 = model(input_ids2)\n",
    "\n",
    "# Les vecteurs de mots sont les dernières représentations cachées\n",
    "word_vectors1 = outputs1.last_hidden_state\n",
    "word_vectors2 = outputs2.last_hidden_state\n",
    "\n",
    "# Calculez les vecteurs moyens pour chaque groupe de mots\n",
    "mean_vector1 = torch.mean(word_vectors1, dim=1)\n",
    "mean_vector2 = torch.mean(word_vectors2, dim=1)\n",
    "\n",
    "# Supprimez les dimensions de taille 1\n",
    "mean_vector1 = mean_vector1.squeeze()\n",
    "mean_vector2 = mean_vector2.squeeze()\n",
    "\n",
    "# Calculez la similarité cosinus entre les deux vecteurs moyens\n",
    "similarity = 1 - cosine(mean_vector1.detach().numpy(), mean_vector2.detach().numpy())\n",
    "\n",
    "print(f\"La similarité entre 'groupe de mots 1' et 'groupe de mots 2' est {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//////////////////////////////////////////////////////////////////////////////////////////////////////////////////  \n",
    "/////////////////////////////////////////////////////////////////////////////////////////////////////////////////  \n",
    "////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de publications récupérées : 12388\n"
     ]
    }
   ],
   "source": [
    "# Script de Waly modifié.\n",
    "# Liste des noms d'auteurs que vous souhaitez rechercher\n",
    "auteurs = [\n",
    "    \"Anne-Virginie Salsac\",\n",
    "    \"Dan Istrate\",\n",
    "    \"Eric Leclerc\",\n",
    "    \"Julien \\\"Le Duigou\\\"\",\n",
    "    \"Marie-Christine \\\"Ho Ba Tho\\\"\",\n",
    "    \"Sofiane Boudaoud\",\n",
    "    \"Glenn Roe\",\n",
    "    \"Motasem ALRAHABI\",\n",
    "    \"Arnaud Latil\",\n",
    "    \"Christian NERI\",\n",
    "    \"Clément Mabi\",\n",
    "    \"David Flacher\",\n",
    "    \"M. Shawky\",\n",
    "    \"Serge Bouchardon\",\n",
    "    \"Harry Sokol\",\n",
    "    \"Bérangère Bihan-Avalle\",\n",
    "    \"Caroline Marti\",\n",
    "    \"Laurent Petit\",\n",
    "    \"Pierre-Carl Langlais\",\n",
    "    \"David Klatzmann\",\n",
    "    \"Raphael Gavazzi\",\n",
    "    \"Benjamin Wandelt\",\n",
    "    \"christophe pichon\",\n",
    "    \"Guilhem Lavaux\",\n",
    "    \"Henry Joy McCracken\",\n",
    "    \"Kumiko Kotera\",\n",
    "    \"Yohan Dubois\",\n",
    "    \"A. Marco Saitta\",\n",
    "    \"Dirk Stratmann\",\n",
    "    \"Guillaume Ferlat\",\n",
    "    \"Slavica Jonic\",\n",
    "    \"Alex Chin\",\n",
    "    \"Fabrice Carrat\",\n",
    "    \"Pierre-Yves Boëlle\",\n",
    "    \"Renaud Piarroux\",\n",
    "    \"Christophe Guillotel-Nothmann\",\n",
    "    \"Jean-Marc Chouvel\",\n",
    "    \"Nicolas Obin\",\n",
    "    \"Philippe Esling\",\n",
    "    \"Alexandre Coninx\",\n",
    "    \"Baptiste Caramiaux\",\n",
    "    \"Benjamin Piwowarski\",\n",
    "    \"Catherine Achard\",\n",
    "    \"Catherine Pelachaud\",\n",
    "    \"Gilles Bailly\",\n",
    "    \"Jérôme Szewczyk\",\n",
    "    \"Kevin Bailly\",\n",
    "    \"Laure Soulier\",\n",
    "    \"Marie-Aude Vitrani\",\n",
    "    \"Matthieu Cord\",\n",
    "    \"Mehdi Khamassi\",\n",
    "    \"Mohamed CHETOUANI\",\n",
    "    \"Nathanaël Jarrassé\",\n",
    "    \"Nicolas Bredeche\",\n",
    "    \"Nicolas Perrin-Gilbert\",\n",
    "    \"Nicolas Thome\",\n",
    "    \"Olivier Schwander\",\n",
    "    \"Olivier Sigaud\",\n",
    "    \"Pascal Morin\",\n",
    "    \"Pierre Bessière\",\n",
    "    \"Sinan Haliyo\",\n",
    "    \"Stéphane Doncieux\",\n",
    "    \"Alessandra Carbone\",\n",
    "    \"Elodie Laine\",\n",
    "    \"Martin Weigt\",\n",
    "    \"Benoit Semelin\",\n",
    "    \"Emeric Bron\",\n",
    "    \"Emmanuel Bertin\",\n",
    "    \"Françoise Combes\",\n",
    "    \"Maryvonne Gerin\",\n",
    "    \"Philippe Salomé\",\n",
    "    \"Baptiste Cecconi\",\n",
    "    \"Philippe Zarka\",\n",
    "    \"Ferdinand Dhombres\",\n",
    "    \"Jean Charlet\",\n",
    "    \"Xavier Tannier\",\n",
    "    \"Amal \\\"El Fallah Seghrouchni\\\"\",\n",
    "    \"Andrea Pinna\",\n",
    "    \"Antoine Miné\",\n",
    "    \"Béatrice Bérard\",\n",
    "    \"Bertrand Granado\",\n",
    "    \"Bruno Escoffier\",\n",
    "    \"Carola Doerr\",\n",
    "    \"Christoph Dürr\",\n",
    "    \"Christophe Denis\",\n",
    "    \"Christophe Marsala\",\n",
    "    \"Colette Faucher\",\n",
    "    \"Emmanuel HYON\",\n",
    "    \"Emmanuelle Encrenaz-Tiphène\",\n",
    "    \"Evripidis Bampis\",\n",
    "    \"Fanny Pascual\",\n",
    "    \"Haralampos Stratigopoulos\",\n",
    "    \"Jean-Daniel Kant\",\n",
    "    \"Jean-Gabriel Ganascia\",\n",
    "    \"Jean-Noël Vittaut\",\n",
    "    \"Lionel Tabourier\",\n",
    "    \"Maria Potop-Butucaru\",\n",
    "    \"Matthieu Latapy\",\n",
    "    \"Nicolas MAUDET\",\n",
    "    \"Olivier Spanjaard\",\n",
    "    \"Patrice Perny\",\n",
    "    \"Patrick Gallinari\",\n",
    "    \"Sébastien Tixeuil\",\n",
    "    \"Spyros Angelopoulos\",\n",
    "    \"Stéphane Gançarski\",\n",
    "    \"Vanda Luengo\",\n",
    "    \"Vincent Guigue\",\n",
    "    \"Bruno Despres\",\n",
    "    \"Frédéric Nataf\",\n",
    "    \"Julien Brajard\",\n",
    "    \"Sylvie Thiria\",\n",
    "    \"Catherine Matias\",\n",
    "    \"Charlotte Dion-Blanc\",\n",
    "    \"Claire Boyer\",\n",
    "    \"Gérard Biau\",\n",
    "    \"Gregory Nuel\",\n",
    "    \"Idris Kharroubi\",\n",
    "    \"Maud Thomas\",\n",
    "    \"Maxime Sangnier\",\n",
    "    \"Olivier Lopez\",\n",
    "    \"Sylvain Le-Corff\",\n",
    "    \"Tabea Rebafka\",\n",
    "    \"Benjamin Fuks\",\n",
    "    \"Stéphane Mottelet\",\n",
    "    \"Tien-Tuan Dao\",\n",
    "    \"julien mozziconacci\",\n",
    "    \"Nicolas Aunai\",\n",
    "    \"Thierry Dufour\",\n",
    "    \"Abdenour Hadid\",\n",
    "    \"Benjamin Quost\",\n",
    "    \"Bruno Toupance\",\n",
    "    \"Dominique Lenne\",\n",
    "    \"Evelyne Heyer\",\n",
    "    \"Franz Manni\",\n",
    "    \"Grace Younes\",\n",
    "    \"Lama Tarsissi\",\n",
    "    \"Marie-Hélène (Mylène) Masson\",\n",
    "    \"Marie-Hélène Abel\",\n",
    "    \"Nathalie Martial-Braz\",\n",
    "    \"Nicolas Patin\",\n",
    "    \"Philippe Bonnifait\",\n",
    "    \"Philippe Boulanger\",\n",
    "    \"Philippe Trigano\",\n",
    "    \"Raed Abu Zitar\",\n",
    "    \"Samuel F. Feng\",\n",
    "    \"Sébastien Destercke\",\n",
    "    \"Tanujit Chakraborty\",\n",
    "    \"Yves Grandvalet\",\n",
    "    \"Zoheir ABOURA\"\n",
    "]\n",
    "\n",
    "# Initialisez une liste vide pour stocker les métadonnées des publications\n",
    "publications = []\n",
    "\n",
    "# URL de base de l'API HAL\n",
    "base_url = \"https://api.archives-ouvertes.fr/search/\"\n",
    "\n",
    "# Parcourez la liste des auteurs et récupérez leurs publications\n",
    "for auteur in auteurs:\n",
    "    params = {\n",
    "        \"q\": f'authFullName_s:\"{auteur}\"',\n",
    "        \"fl\": \"authFullName_s,authIdHal_i,authIdHal_s,title_s,halId_s,producedDateY_i,doiId_s,abstract_s,uri_s,domain_s,keyword_s\",\n",
    "        \"rows\": 10000  # Augmentez le nombre de lignes si nécessaire\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        publications.extend(data[\"response\"][\"docs\"])\n",
    "    else:\n",
    "        print(f\"Erreur pour l'auteur {auteur}: {response.status_code} - {response.text}\")\n",
    "\n",
    "# Créez un DataFrame Pandas à partir des métadonnées des publications\n",
    "column_order = [\"authFullName_s\", \"authIdHal_i\", \"authIdHal_s\", \"title_s\", \"halId_s\", \"producedDateY_i\", \"doiId_s\", \"keyword_s\", \"abstract_s\", \"uri_s\", \"domain_s\"]\n",
    "\n",
    "df = pd.DataFrame(publications, columns= column_order)\n",
    "\n",
    "# Sauvegardez le DataFrame dans un fichier CSV\n",
    "#df.to_csv(\"publications_hal_scai_complet.csv\", index=False)\n",
    "\n",
    "# Affichez le nombre total de publications récupérées\n",
    "print(f\"Nombre total de publications récupérées : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publications restantes après nettoyage : 4206\n"
     ]
    }
   ],
   "source": [
    "# Conversion les listes en chaînes dans la colonne \"title_s\"\n",
    "df[\"title_s\"] = df[\"title_s\"].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Suppression des doublons\n",
    "df = df.drop_duplicates(subset=\"title_s\")\n",
    "dfp = df.dropna(subset=[\"abstract_s\", \"keyword_s\"])\n",
    "print(f\"Publications restantes après nettoyage : {len(dfp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Détection de la langue terminée \n",
      "\n",
      "Nombre d'articles anglais :\n",
      "3547\n",
      "Nombre d'articles francais :\n",
      "654\n",
      "Nombre d'articles allemand :\n",
      "2\n",
      "Index des articles avec langue inconnue\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "dfp = dfp.copy()\n",
    "dfp['language'] = 'unknown'\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "def lang(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i,'language'] = detect_language(str(df.loc[i,'abstract_s']))\n",
    "    print(\"Détection de la langue terminée \\n\")\n",
    "    return df\n",
    "\n",
    "dfp = lang(dfp)\n",
    "\n",
    "def counter(df):\n",
    "    counts = df['language'].value_counts()\n",
    "    print(\"Nombre d'articles anglais :\")\n",
    "    print(counts.get('en', 0))\n",
    "\n",
    "    print(\"Nombre d'articles francais :\")\n",
    "    print(counts.get('fr', 0))\n",
    "\n",
    "    print(\"Nombre d'articles allemand :\")\n",
    "    print(counts.get('de', 0))\n",
    "\n",
    "    print(\"Index des articles avec langue inconnue\")\n",
    "    print(df[df['language'] == 'UNKNOWN'].index.tolist())\n",
    "    return\n",
    "\n",
    "counter(dfp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID de l'auteur\n",
    "author_id = \"a5007814380\"  \n",
    "\n",
    "# URL de l'API\n",
    "url = f\"https://api.openalex.org/works?filter=author.id:{author_id}\"\n",
    "\n",
    "# Faire la requête à l'API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Vérifier que la requête a réussi\n",
    "if response.status_code == 200:\n",
    "    # Conversion de la réponse en JSON\n",
    "    data = json.loads(response.text)\n",
    "\n",
    "    # Sélection les métadonnées précises\n",
    "    selected_data = []\n",
    "    \n",
    "    for work in data['results']:\n",
    "        \n",
    "        author_ids = []\n",
    "        if 'authorships' in work: \n",
    "            for authorship in work['authorships']:\n",
    "                # Ajoutez l'identifiant de l'auteur à la liste 'author_ids'\n",
    "                author_ids.append(authorship[\"author\"][\"id\"])\n",
    "                \n",
    "        selected_data.append({\n",
    "            'id' : work['id'],\n",
    "            'title': work['title'],\n",
    "            'authors_id' : author_ids, \n",
    "            'referenced_works': work['referenced_works'], \n",
    "            'related_works': work['related_works'], \n",
    "            'cited_by_api_url': work['cited_by_api_url'],    \n",
    "        })\n",
    "\n",
    "    # Conversion des données sélectionnées en DataFrame\n",
    "    df = pd.DataFrame(selected_data)\n",
    "\n",
    "    # Exportation du DataFrame en CSV\n",
    "    df.to_csv('meta_pub_oa.csv', index=False)\n",
    "else:\n",
    "    print(f\"Erreur : {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auteurs = [\n",
    "    \"a5063079937\",\n",
    "    \"a5064456106\",\n",
    "    \"a5075692514\",\n",
    "    \"a5076657262\",\n",
    "    \"a5060263799\",\n",
    "    \"a5059667793\",\n",
    "    \"a5010298238\",\n",
    "    \"a5009962581\",\n",
    "    \"a5019206372\",\n",
    "    \"a5026117716\",\n",
    "    \"Clément Mabi\",\n",
    "    \"a5045653236\", #David Flacher\n",
    "    # pas là \"M. Shawky\",\n",
    "    \"a5054128778\", #Serge Bouchardon \n",
    "    \"a5083701333\",\n",
    "    #Bérangère Bihan-Avalle\n",
    "    #Caroline Marti\n",
    "    \"a5043441322\", #Laurent Petit\n",
    "    \"a5065665642\", # ou  Pierre-Carl Langlais a5090355875 \n",
    "    \"a5044456322\",\n",
    "    \"a5072965626\",\n",
    "    \"a5050309898\",\n",
    "    \"a5022276793\",\n",
    "    \"a5035893506\", #Guilhem Lavaux, y en a 2 donc un dechet\n",
    "    #\"Henry Joy McCracken\", chelou\n",
    "    \"a5051397820\",\n",
    "    \"a5075521248\", # Yohan Dubois en x2\n",
    "    \"a5048168479\",\n",
    "    \"a5008261497\",\n",
    "    \"a5006372418\",\n",
    "    # existe pas \"Slavica Jonic\",\n",
    "    \"a5032837587\",\n",
    "    \"a5082959305\",\n",
    "    \"a5000035181\", # y en a 2?? \"Pierre-Yves Boëlle\",\n",
    "    \"a5008914363\", # y en a 2 \"Renaud Piarroux\",\n",
    "    \"a5012443434\",\n",
    "    \"a5086118723\", # Jean marc chouvel y en a 3\n",
    "    \"a5042745853\",\n",
    "    \"a5085170922\",\n",
    "    \"a5073978648\",\n",
    "    \"a5060825472\",\n",
    "    \"a5009145141\", # 2 differents \"Benjamin Piwowarski\",\n",
    "    \"a5005922530\", # 2 a cause du nom \"Catherine Achard\",\n",
    "    \"a5079026902\", # 2 a cause du nom \"Catherine Pelachaud\",\n",
    "    \"a5019033794\", # plusieurs \"Gilles Bailly\",\n",
    "    \"a5084974660\",\n",
    "    \"a5015493311\",\n",
    "    \"a5000942708\", # 2 differents \"Laure Soulier\",\n",
    "    \"a5024969402\",\n",
    "    \"a5022871131\", # 2 differents Matthieu Cord\n",
    "    \"a5016381589\",\n",
    "    \"a5049398785\",\n",
    "    \"a5057665161\", # 2 différents \"Nathanaël Jarrassé\",\n",
    "    \"a5008211251\",\n",
    "    \"a5047587289\",\n",
    "    \"a5017490804\",\n",
    "    \"a5018812634\",\n",
    "    \"a5042850624\",\n",
    "    \"a5020385150\", # 2 differents importants \"Pascal Morin\",\n",
    "    \"a5053141872\", # 2 differents \"Pierre Bessière\",\n",
    "    \"a5016542830\", # 2 differents \"Sinan Haliyo\",\n",
    "    \"a5003629424\",\n",
    "    \"a5038174177\", # 2 differents importants  \"Alessandra Carbone\" \n",
    "    \"a5076453514\", # 2 differents \"Elodie Laine\",\n",
    "    \"a5035771024\",\n",
    "    \"a5049132515\",\n",
    "    \"a5007007631\",\n",
    "    \"a5089708177\", # doute \"Emmanuel Bertin\",\n",
    "    \"a5064812526\",\n",
    "    \"a5008102443\", # 2 differents \"Maryvonne Gerin\",\n",
    "    \"a5061694871\",\n",
    "    \"a5032663932\",\n",
    "    \"a5074319502\", # 2 differents \" Philippe Zarka\",\n",
    "    \"a5005420349\",\n",
    "    \"a5055383240\",\n",
    "    \"a5056834851\", # 2 differents \" Xavier Tannier\",\n",
    "    \"a5044546919\", # plusieurs differents importants  \" Amal El Fallah-Seghrouchni \" \n",
    "    \"a5019342840\",\n",
    "    \"a5069148908\",\n",
    "    \"a5008504744\",\n",
    "    \"a5057674250\",\n",
    "    \"a5020410268\",\n",
    "    \"a5040561209\",\n",
    "    \"a5029270613\",\n",
    "    \"a5003029415\",\n",
    "    \"a5076504951\", # 2 differents \" Christophe Marsala\",\n",
    "    \"a5008947610\",\n",
    "    \"a5062847773\", # 2 differents \" Emmanuel Hyon \"\n",
    "    \"a5040506059\",\n",
    "    \"a5063171222\", # 2 differents \"Evripidis Bampis \"\n",
    "    \"a5031467845\",\n",
    "    \"a5091734149\",\n",
    "    \"a5020645950\", # 2 differents \" Jean-Daniel Kant \"\n",
    "    \"a5052738299\", # 2 differents \"Jean‐Gabriel Ganascia \"\n",
    "    \"a5009688030\",\n",
    "    \"a5056981393\",\n",
    "    \"a5080217489\", # 2 differents Maria Potop-Butucaru \n",
    "    \"a5031952531\", # 2 differents Matthieu Latapy  \n",
    "    \"a5000925369\",\n",
    "    \"a5012670875\",\n",
    "    \"a5025612115\",\n",
    "    \"a5086752907\",\n",
    "    \"a5073883755\", # 2 differents Sébastien Tixeuil \n",
    "    \"a5063338404\", # 2 differents Spyros Angelopoulos \n",
    "    \"a5049035512\",\n",
    "    \"a5073711335\",\n",
    "    \"a5044389669\",\n",
    "    \"a5090273130\",\n",
    "    \"a5004798802\",\n",
    "    \"a5073300655\", # 3 differents Julien Brajard\n",
    "    \"a5042022387\", # 2 differents Sylvie Thiria \n",
    "    \"a5082187550\",\n",
    "    \"a5011096724\",\n",
    "    \"a5010301554\",\n",
    "    \"a5007814380\",\n",
    "    \"a5087944743\",\n",
    "    \"a5051885232\",# 2 differents Idris Kharroubi \n",
    "    \"a5076190399\",# 2 differents Maud Thomas \n",
    "    \"a5043113193\",\n",
    "    \"a5041522495\", # 3 differents importants Olivier Lopez\n",
    "    \"a5049226969\",\n",
    "    \"a5011379612\",\n",
    "    \"a5029271059\",\n",
    "    \"a5043406295\",\n",
    "    \"a5051516688\", #  Tien Tuan Dao  pas de mention SU\n",
    "    \"a5027346778\",\n",
    "    \"a5040717345\",\n",
    "    \"a5048594819\", # 2 differents AThierry Dufour \n",
    "    \"a5013928164\", # 2 differents Abdenour Hadid \n",
    "    \"a5090882219\",\n",
    "    \"a5024260775\",\n",
    "    \"a5051963748\",\n",
    "    \"a5005868901\",\n",
    "    \"a5044064042\", # 2 differents  Franz Manni  \n",
    "    \"a5027364763\", # 2 differents Grace Younes \n",
    "    \"a5033136511\",\n",
    "    \"a5076437822\",\n",
    "    \"a5064018319\",\n",
    "    \"a5049226428\",\n",
    "    \"a5000802564\",\n",
    "    \"a5050418541\", # 2 differents Philippe Bonnifait \n",
    "    \"a5085744367\",\n",
    "    \"a5054524844\", # pas de mention de SU Philippe Trigano\n",
    "    \"a5078607983\", # 3 differents  Raed Abu Zitar \n",
    "    \"a5008158634\",\n",
    "    \"a5070285963\",\n",
    "    \"a5012926469\",\n",
    "    \"a5021351429\",\n",
    "    \"a5009983613\"\n",
    "]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
